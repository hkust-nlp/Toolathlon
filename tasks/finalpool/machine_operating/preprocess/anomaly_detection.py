#!/usr/bin/env python3
"""
å¼‚å¸¸æ£€æµ‹è„šæœ¬ - è¯†åˆ«ä¼ æ„Ÿå™¨æ•°æ®ä¸­çš„å¼‚å¸¸è¯»æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰

åŠŸèƒ½ï¼š
1. ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´çš„ä¼ æ„Ÿå™¨æ•°æ®
2. ç»“åˆExcelå‚æ•°é…ç½®è¯†åˆ«å¼‚å¸¸
3. ç”Ÿæˆå¼‚å¸¸æŠ¥å‘Š
4. æ”¯æŒå¤šæ•°æ®é›†å¤„ç†å’Œçµæ´»é…ç½®
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import argparse
import glob
from pathlib import Path

def find_data_files(prefix=""):
    """æŸ¥æ‰¾æ•°æ®æ–‡ä»¶"""
    if prefix and not prefix.endswith('_'):
        prefix += '_'
    
    sensor_pattern = f"{prefix}live_sensor_data.csv"
    params_pattern = f"{prefix}machine_operating_parameters.xlsx"
    
    sensor_files = glob.glob(sensor_pattern)
    params_files = glob.glob(params_pattern)
    
    if not sensor_files:
        # å°è¯•æŸ¥æ‰¾ä»»ä½•ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶
        all_sensor_files = glob.glob("*live_sensor_data.csv")
        if all_sensor_files:
            print(f"æœªæ‰¾åˆ° {sensor_pattern}ï¼Œå¯ç”¨çš„ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶ï¼š")
            for i, file in enumerate(all_sensor_files):
                print(f"  {i+1}. {file}")
            return None, None
        else:
            print("æœªæ‰¾åˆ°ä»»ä½•ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶")
            return None, None
    
    if not params_files:
        print(f"æœªæ‰¾åˆ°å‚æ•°é…ç½®æ–‡ä»¶: {params_pattern}")
        return None, None
    
    return sensor_files[0], params_files[0]

def load_data(prefix=""):
    """åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®å’Œå‚æ•°é…ç½®"""
    print("æŸ¥æ‰¾å’ŒåŠ è½½æ•°æ®æ–‡ä»¶...")
    
    sensor_file, params_file = find_data_files(prefix)
    if not sensor_file or not params_file:
        return None, None
    
    print(f"ä½¿ç”¨ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶: {sensor_file}")
    print(f"ä½¿ç”¨å‚æ•°é…ç½®æ–‡ä»¶: {params_file}")
    
    # åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®
    sensor_data = pd.read_csv(sensor_file)
    sensor_data['timestamp'] = pd.to_datetime(sensor_data['timestamp'])
    
    # åŠ è½½å‚æ•°é…ç½®
    params_data = pd.read_excel(params_file, sheet_name='Operating Parameters')
    
    print(f"ä¼ æ„Ÿå™¨æ•°æ®è®°å½•æ•°: {len(sensor_data):,}")
    print(f"å‚æ•°é…ç½®è®°å½•æ•°: {len(params_data):,}")
    print(f"æ•°æ®æ—¶é—´èŒƒå›´: {sensor_data['timestamp'].min()} åˆ° {sensor_data['timestamp'].max()}")
    print(f"æœºå™¨æ•°é‡: {sensor_data['machine_id'].nunique()}")
    print(f"ä¼ æ„Ÿå™¨ç±»å‹: {sensor_data['sensor_type'].nunique()} ({', '.join(sorted(sensor_data['sensor_type'].unique()))})")
    
    return sensor_data, params_data

def parse_time_input(time_str):
    """è§£ææ—¶é—´è¾“å…¥ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if not time_str:
        return None
    
    # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼
    time_formats = [
        '%H:%M',           # 11:30
        '%H:%M:%S',        # 11:30:00
        '%Y-%m-%d %H:%M',  # 2024-08-19 11:30
        '%Y-%m-%d %H:%M:%S',  # 2024-08-19 11:30:00
        '%m-%d %H:%M',     # 08-19 11:30
    ]
    
    for fmt in time_formats:
        try:
            return datetime.strptime(time_str, fmt)
        except ValueError:
            continue
    
    raise ValueError(f"æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_str}")

def filter_time_range(data, start_time=None, end_time=None):
    """ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®"""
    if not start_time and not end_time:
        print("æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
        return data
    
    print(f"\nç­›é€‰æ—¶é—´èŒƒå›´: {start_time or 'æ•°æ®å¼€å§‹'} åˆ° {end_time or 'æ•°æ®ç»“æŸ'}")
    
    filtered_data = data.copy()
    
    if start_time:
        try:
            start_dt = parse_time_input(start_time)
            
            # å¦‚æœåªæœ‰æ—¶é—´æ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨æ•°æ®ä¸­çš„æ—¥æœŸ
            if start_dt.date() == datetime(1900, 1, 1).date():
                data_date = data['timestamp'].dt.date.iloc[0]
                start_dt = datetime.combine(data_date, start_dt.time())
            
            filtered_data = filtered_data[filtered_data['timestamp'] >= start_dt]
            
        except ValueError as e:
            print(f"å¼€å§‹æ—¶é—´è§£æé”™è¯¯: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•åªæ¯”è¾ƒæ—¶é—´éƒ¨åˆ†
            try:
                start_time_obj = datetime.strptime(start_time, '%H:%M').time()
                filtered_data = filtered_data[filtered_data['timestamp'].dt.time >= start_time_obj]
            except ValueError:
                print("ä½¿ç”¨é»˜è®¤å¼€å§‹æ—¶é—´")
    
    if end_time:
        try:
            end_dt = parse_time_input(end_time)
            
            # å¦‚æœåªæœ‰æ—¶é—´æ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨æ•°æ®ä¸­çš„æ—¥æœŸ
            if end_dt.date() == datetime(1900, 1, 1).date():
                data_date = data['timestamp'].dt.date.iloc[-1]
                end_dt = datetime.combine(data_date, end_dt.time())
            
            filtered_data = filtered_data[filtered_data['timestamp'] <= end_dt]
            
        except ValueError as e:
            print(f"ç»“æŸæ—¶é—´è§£æé”™è¯¯: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•åªæ¯”è¾ƒæ—¶é—´éƒ¨åˆ†
            try:
                end_time_obj = datetime.strptime(end_time, '%H:%M').time()
                filtered_data = filtered_data[filtered_data['timestamp'].dt.time <= end_time_obj]
            except ValueError:
                print("ä½¿ç”¨é»˜è®¤ç»“æŸæ—¶é—´")
    
    print(f"ç­›é€‰åæ•°æ®è®°å½•æ•°: {len(filtered_data):,}")
    
    if len(filtered_data) > 0:
        print(f"å®é™…æ—¶é—´èŒƒå›´: {filtered_data['timestamp'].min()} åˆ° {filtered_data['timestamp'].max()}")
    
    return filtered_data

def detect_anomalies(sensor_data, params_data):
    """æ£€æµ‹å¼‚å¸¸è¯»æ•°"""
    print("\nå¼€å§‹å¼‚å¸¸æ£€æµ‹...")
    
    # åˆå¹¶æ•°æ®
    merged_data = sensor_data.merge(
        params_data[['machine_id', 'sensor_type', 'min_value', 'max_value', 'unit']], 
        on=['machine_id', 'sensor_type'],
        how='left'
    )
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªåŒ¹é…çš„æ•°æ®
    unmatched = merged_data[merged_data['min_value'].isna()]
    if len(unmatched) > 0:
        print(f"è­¦å‘Š: æœ‰ {len(unmatched)} æ¡æ•°æ®æœªæ‰¾åˆ°å¯¹åº”çš„å‚æ•°é…ç½®")
    
    # è¯†åˆ«å¼‚å¸¸
    merged_data['is_below_min'] = merged_data['reading'] < merged_data['min_value']
    merged_data['is_above_max'] = merged_data['reading'] > merged_data['max_value']
    merged_data['is_anomaly'] = merged_data['is_below_min'] | merged_data['is_above_max']
    
    # ç­›é€‰å¼‚å¸¸æ•°æ®
    anomalies = merged_data[merged_data['is_anomaly']].copy()
    
    # æ·»åŠ å¼‚å¸¸ç±»å‹å’Œæ­£å¸¸èŒƒå›´ä¿¡æ¯
    anomalies['anomaly_type'] = anomalies.apply(
        lambda row: 'below_minimum' if row['is_below_min'] else 'above_maximum', 
        axis=1
    )
    anomalies['normal_range'] = (
        anomalies['min_value'].astype(str) + ' - ' + 
        anomalies['max_value'].astype(str) + ' ' + 
        anomalies['unit'].astype(str)
    )
    
    print(f"æ£€æµ‹åˆ°å¼‚å¸¸æ•°æ®: {len(anomalies)} æ¡")
    print(f"å¼‚å¸¸ç‡: {len(anomalies) / len(merged_data) * 100:.1f}%")
    
    return anomalies, merged_data

def generate_anomaly_report(anomalies, output_prefix=""):
    """ç”Ÿæˆå¼‚å¸¸æŠ¥å‘Š"""
    print("\nç”Ÿæˆå¼‚å¸¸æŠ¥å‘Š...")
    
    if len(anomalies) == 0:
        print("âš ï¸ æœªå‘ç°å¼‚å¸¸æ•°æ®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
        return pd.DataFrame(), None
    
    # é€‰æ‹©æŠ¥å‘Šå­—æ®µ
    report_columns = [
        'timestamp', 'machine_id', 'sensor_type', 'reading', 
        'normal_range', 'anomaly_type', 'unit'
    ]
    
    report = anomalies[report_columns].copy()
    
    # æŒ‰æ—¶é—´æ’åº
    report = report.sort_values('timestamp')
    
    # æ·»åŠ ä¸¥é‡ç¨‹åº¦è¯„åˆ†
    def calculate_severity(row):
        try:
            # è§£æ "min_val - max_val unit" æ ¼å¼çš„å­—ç¬¦ä¸²
            range_parts = row['normal_range'].split(' - ')
            min_val = float(range_parts[0])
            # ç¬¬äºŒéƒ¨åˆ†å¯èƒ½åŒ…å«å•ä½ï¼Œéœ€è¦æå–æ•°å€¼éƒ¨åˆ†
            max_part = range_parts[1].split()[0]  # åªå–ç¬¬ä¸€ä¸ªç©ºæ ¼å‰çš„éƒ¨åˆ†
            max_val = float(max_part)
            
            range_size = max_val - min_val
            if range_size <= 0:  # é¿å…é™¤é›¶é”™è¯¯
                return 'Low'
                
            if row['anomaly_type'] == 'above_maximum':
                deviation = (row['reading'] - max_val) / range_size
            else:
                deviation = (min_val - row['reading']) / range_size
            
            if deviation >= 2.0:
                return 'Critical'
            elif deviation >= 1.0:
                return 'High'
            elif deviation >= 0.5:
                return 'Medium'
            else:
                return 'Low'
        except (ValueError, IndexError, ZeroDivisionError) as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ–¹æ³•åŸºäºå¼‚å¸¸ç±»å‹åˆ¤æ–­
            return 'Medium'
    
    report['severity'] = report.apply(calculate_severity, axis=1)
    
    # ç”Ÿæˆæ–‡ä»¶å
    if output_prefix and not output_prefix.endswith('_'):
        output_prefix += '_'
    
    timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_filename = f'{output_prefix}anomaly_report_{timestamp_str}.csv'
    
    # ä¿å­˜ä¸ºCSV
    report.to_csv(report_filename, index=False)
    
    print(f"å¼‚å¸¸æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")
    print(f"æŠ¥å‘ŠåŒ…å« {len(report)} æ¡å¼‚å¸¸è®°å½•")
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
    severity_counts = report['severity'].value_counts()
    print("ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
    for severity, count in severity_counts.items():
        print(f"  {severity}: {count} æ¡")
    
    return report, report_filename

def print_summary_statistics(anomalies, all_data):
    """æ‰“å°æ±‡æ€»ç»Ÿè®¡"""
    print("\n" + "="*60)
    print("å¼‚å¸¸æ£€æµ‹æ±‡æ€»ç»Ÿè®¡")
    print("="*60)
    
    if len(anomalies) == 0:
        print("æœªæ£€æµ‹åˆ°ä»»ä½•å¼‚å¸¸æ•°æ®")
        return
    
    # æ€»ä½“ç»Ÿè®¡
    total_records = len(all_data)
    anomaly_count = len(anomalies)
    anomaly_rate = anomaly_count / total_records * 100
    
    print(f"æ€»æ•°æ®è®°å½•æ•°: {total_records:,}")
    print(f"å¼‚å¸¸æ•°æ®è®°å½•æ•°: {anomaly_count:,}")
    print(f"å¼‚å¸¸ç‡: {anomaly_rate:.1f}%")
    
    # æŒ‰æœºå™¨ç»Ÿè®¡
    print(f"\nå„æœºå™¨å¼‚å¸¸ç»Ÿè®¡:")
    machine_stats = anomalies.groupby('machine_id').agg({
        'timestamp': 'count',
        'reading': 'count'
    }).rename(columns={'timestamp': 'anomaly_count'})
    
    # è®¡ç®—å„æœºå™¨çš„æ€»æ•°æ®é‡
    total_by_machine = all_data.groupby('machine_id').size()
    machine_stats['total_records'] = total_by_machine
    machine_stats['anomaly_rate'] = (machine_stats['anomaly_count'] / machine_stats['total_records'] * 100).round(1)
    
    print(machine_stats[['anomaly_count', 'anomaly_rate']].to_string())
    
    # æŒ‰ä¼ æ„Ÿå™¨ç±»å‹ç»Ÿè®¡
    print(f"\nå„ä¼ æ„Ÿå™¨ç±»å‹å¼‚å¸¸ç»Ÿè®¡:")
    sensor_stats = anomalies.groupby('sensor_type').agg({
        'timestamp': 'count',
        'reading': 'count'
    }).rename(columns={'timestamp': 'anomaly_count'})
    
    total_by_sensor = all_data.groupby('sensor_type').size()
    sensor_stats['total_records'] = total_by_sensor
    sensor_stats['anomaly_rate'] = (sensor_stats['anomaly_count'] / sensor_stats['total_records'] * 100).round(1)
    
    print(sensor_stats[['anomaly_count', 'anomaly_rate']].to_string())
    
    # æŒ‰å¼‚å¸¸ç±»å‹ç»Ÿè®¡
    print(f"\nå¼‚å¸¸ç±»å‹åˆ†å¸ƒ:")
    anomaly_type_stats = anomalies['anomaly_type'].value_counts()
    print(anomaly_type_stats.to_string())
    
    # æœ€ä¸¥é‡çš„å¼‚å¸¸
    print(f"\næœ€ä¸¥é‡çš„å¼‚å¸¸è¯»æ•° (å‰10æ¡):")
    
    # è®¡ç®—åç¦»æ­£å¸¸èŒƒå›´çš„ç¨‹åº¦
    anomalies_copy = anomalies.copy()
    anomalies_copy['deviation'] = anomalies_copy.apply(
        lambda row: abs(row['reading'] - row['max_value']) if row['anomaly_type'] == 'above_maximum' 
        else abs(row['min_value'] - row['reading']), axis=1
    )
    
    top_anomalies = anomalies_copy.nlargest(10, 'deviation')[
        ['timestamp', 'machine_id', 'sensor_type', 'reading', 'normal_range', 'deviation']
    ]
    
    print(top_anomalies.to_string(index=False))

def print_sample_anomalies(anomalies, n=15):
    """æ‰“å°å¼‚å¸¸æ ·æœ¬"""
    print(f"\nå¼‚å¸¸æ•°æ®æ ·æœ¬ (å‰{n}æ¡):")
    
    if len(anomalies) == 0:
        print("æ— å¼‚å¸¸æ•°æ®")
        return
    
    sample = anomalies.head(n)[
        ['timestamp', 'machine_id', 'sensor_type', 'reading', 'normal_range', 'anomaly_type']
    ]
    
    print(sample.to_string(index=False))

def show_dataset_overview(sensor_data):
    """æ˜¾ç¤ºæ•°æ®é›†æ¦‚è§ˆ"""
    print(f"\nğŸ“Š æ•°æ®é›†æ¦‚è§ˆ:")
    print(f"="*50)
    
    # æ—¶é—´èŒƒå›´
    time_span = sensor_data['timestamp'].max() - sensor_data['timestamp'].min()
    print(f"ğŸ“… æ—¶é—´è·¨åº¦: {time_span}")
    
    # é‡‡æ ·é¢‘ç‡
    time_diffs = sensor_data['timestamp'].diff().dropna()
    avg_interval = time_diffs.median()
    print(f"â° å¹³å‡é‡‡æ ·é—´éš”: {avg_interval}")
    
    # æœºå™¨å’Œä¼ æ„Ÿå™¨ç»Ÿè®¡
    machines = sensor_data['machine_id'].unique()
    sensors = sensor_data['sensor_type'].unique()
    
    print(f"ğŸ­ æœºå™¨æ•°é‡: {len(machines)}")
    print(f"ğŸ“¡ ä¼ æ„Ÿå™¨ç±»å‹: {len(sensors)}")
    print(f"   ç±»å‹: {', '.join(sorted(sensors))}")
    
    # æ•°æ®å®Œæ•´æ€§
    expected_records = len(machines) * len(sensors) * len(sensor_data['timestamp'].unique())
    actual_records = len(sensor_data)
    completeness = (actual_records / expected_records) * 100
    
    print(f"âœ… æ•°æ®å®Œæ•´æ€§: {completeness:.1f}% ({actual_records:,}/{expected_records:,})")
    
    # æ•°æ®è´¨é‡
    null_count = sensor_data.isnull().sum().sum()
    print(f"âŒ ç¼ºå¤±å€¼æ•°é‡: {null_count}")
    
    if null_count == 0:
        print("ğŸ‰ æ•°æ®è´¨é‡: ä¼˜ç§€ï¼ˆæ— ç¼ºå¤±å€¼ï¼‰")
    elif null_count < actual_records * 0.01:
        print("ğŸ‘ æ•°æ®è´¨é‡: è‰¯å¥½ï¼ˆç¼ºå¤±å€¼ < 1%ï¼‰")
    else:
        print("âš ï¸ æ•°æ®è´¨é‡: éœ€è¦å…³æ³¨ï¼ˆç¼ºå¤±å€¼è¾ƒå¤šï¼‰")

def list_available_datasets():
    """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
    sensor_files = glob.glob("*live_sensor_data.csv")
    
    if not sensor_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶")
        return []
    
    print("ğŸ“ å¯ç”¨çš„æ•°æ®é›†:")
    datasets = []
    
    for i, file in enumerate(sensor_files):
        # æå–å‰ç¼€
        prefix = file.replace('live_sensor_data.csv', '').rstrip('_')
        if not prefix:
            prefix = "default"
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(file) / 1024  # KB
        
        # è¯»å–å°‘é‡æ•°æ®è·å–æ¦‚è§ˆ
        try:
            sample_data = pd.read_csv(file, nrows=100)
            machine_count = sample_data['machine_id'].nunique()
            sensor_count = sample_data['sensor_type'].nunique()
            
            print(f"  {i+1}. {prefix:<20} ({file_size:.1f}KB, {machine_count}å°æœºå™¨, {sensor_count}ç§ä¼ æ„Ÿå™¨)")
            datasets.append(prefix)
            
        except Exception as e:
            print(f"  {i+1}. {prefix:<20} ({file_size:.1f}KB, è¯»å–é”™è¯¯: {e})")
    
    return datasets

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='å·¥å‚ç‰©è”ç½‘ä¼ æ„Ÿå™¨å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºç¡€ç”¨æ³•ï¼ˆä½¿ç”¨é»˜è®¤æ•°æ®æ–‡ä»¶ï¼‰
  python anomaly_detection.py
  
  # æŒ‡å®šæ•°æ®é›†å‰ç¼€
  python anomaly_detection.py --prefix large_dataset
  
  # æŒ‡å®šæ—¶é—´èŒƒå›´
  python anomaly_detection.py --start-time "11:30" --end-time "12:30"
  
  # å®Œæ•´æ—¥æœŸæ—¶é—´
  python anomaly_detection.py --start-time "2024-08-19 11:30" --end-time "2024-08-19 12:30"
  
  # æ˜¾ç¤ºå¯ç”¨æ•°æ®é›†
  python anomaly_detection.py --list-datasets
  
  # åªæ˜¾ç¤ºæ¦‚è§ˆï¼Œä¸è¿›è¡Œå¼‚å¸¸æ£€æµ‹
  python anomaly_detection.py --overview-only
        """
    )
    
    parser.add_argument('--prefix', type=str, default='',
                        help='æ•°æ®æ–‡ä»¶å‰ç¼€ï¼Œé»˜è®¤: æ— å‰ç¼€')
    parser.add_argument('--start-time', type=str, default=None,
                        help='å¼€å§‹æ—¶é—´ (æ ¼å¼: HH:MM æˆ– YYYY-MM-DD HH:MM)')
    parser.add_argument('--end-time', type=str, default=None,
                        help='ç»“æŸæ—¶é—´ (æ ¼å¼: HH:MM æˆ– YYYY-MM-DD HH:MM)')
    parser.add_argument('--output-prefix', type=str, default='',
                        help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶å‰ç¼€ï¼Œé»˜è®¤: æ— å‰ç¼€')
    parser.add_argument('--list-datasets', action='store_true',
                        help='åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†')
    parser.add_argument('--overview-only', action='store_true',
                        help='åªæ˜¾ç¤ºæ•°æ®é›†æ¦‚è§ˆï¼Œä¸è¿›è¡Œå¼‚å¸¸æ£€æµ‹')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºæ•°æ®é›†
    if args.list_datasets:
        list_available_datasets()
        return
    
    print("="*80)
    print("ğŸ­ å·¥å‚ç‰©è”ç½‘ä¼ æ„Ÿå™¨å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("="*80)
    
    try:
        # åŠ è½½æ•°æ®
        sensor_data, params_data = load_data(args.prefix)
        
        if sensor_data is None or params_data is None:
            print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --list-datasets æŸ¥çœ‹å¯ç”¨çš„æ•°æ®é›†")
            return
        
        # æ˜¾ç¤ºæ•°æ®é›†æ¦‚è§ˆ
        show_dataset_overview(sensor_data)
        
        # å¦‚æœåªæ˜¾ç¤ºæ¦‚è§ˆ
        if args.overview_only:
            return
        
        # ç­›é€‰æ—¶é—´èŒƒå›´
        filtered_data = filter_time_range(sensor_data, args.start_time, args.end_time)
        
        if len(filtered_data) == 0:
            print("âš ï¸ è­¦å‘Š: æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®")
            return
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies, all_filtered_data = detect_anomalies(filtered_data, params_data)
        
        # ç”ŸæˆæŠ¥å‘Š
        output_prefix = args.output_prefix or args.prefix
        report, report_filename = generate_anomaly_report(anomalies, output_prefix)
        
        if report_filename:
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print_summary_statistics(anomalies, all_filtered_data)
            
            # æ‰“å°å¼‚å¸¸æ ·æœ¬
            print_sample_anomalies(anomalies)
            
            print(f"\n" + "="*80)
            print("ğŸ‰ å¼‚å¸¸æ£€æµ‹å®Œæˆï¼")
            print("="*80)
            print(f"ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶: {report_filename}")
            print(f"ğŸ“Š å¼‚å¸¸æ•°æ®æ€»æ•°: {len(anomalies):,}")
            print("ğŸ’¡ å»ºè®®: å°†å¼‚å¸¸æŠ¥å‘Šä¸Šä¼ è‡³ iot_anomaly_reports äº‘å­˜å‚¨æ¡¶")
        
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ - {e}")
        print("ğŸ’¡ æç¤º:")
        print("   1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   2. ä½¿ç”¨ --list-datasets æŸ¥çœ‹å¯ç”¨æ•°æ®é›†")
        print("   3. ä½¿ç”¨ --prefix æŒ‡å®šæ­£ç¡®çš„æ•°æ®é›†å‰ç¼€")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 