# About Me

## Research Areas
**Main Research Area:** Visual Generative Models

**Specific Fields:** Application and innovation of diffusion models, flow matching and related technologies in tasks such as image generation, video generation, and cross-modal generation.

## Research Interests

My research focuses on advancing the frontier of visual content generation through deep generative models. I am particularly interested in:

- **Diffusion Models & Flow Matching**: Developing efficient sampling strategies and improving the quality-diversity trade-off in generative processes
- **Video Generation**: Exploring temporal consistency and motion dynamics in video synthesis using state-of-the-art generative frameworks
- **Cross-modal Generation**: Bridging different modalities (text-to-image, image-to-video, audio-visual synthesis) through unified generative architectures
- **Controllable Generation**: Investigating methods for fine-grained control over generated content while maintaining high fidelity
- **Efficient Inference**: Optimizing generative models for real-world deployment with reduced computational costs

## News

üîç **I am actively seeking a postdoctoral position at a university in Hong Kong.** If you are interested in collaboration or have opportunities available, please feel free to contact me.

üìß Contact: [tadaawc@adp.com]

## Recent Publications

Please refer to my google scholar page for this part!

## Education

- Ph.D. in CSE, ABU, 2026 (expected)
- M.S. in CSE, ABU, 2021
- B.S. in EE, BCU, 2019

## Selected Projects

### Project 1: Diffusion Models for High-Quality Image Synthesis
Developed an advanced diffusion-based framework that significantly improves the fidelity and resolution of generated images. Introduced novel architectural enhancements and sampling techniques to achieve state-of-the-art results on benchmark datasets.

### Project 2: Consistent Video Generation via Flow Matching
Proposed a flow matching approach for temporal-consistent video generation, enabling the synthesis of realistic motion and persistent object appearances across frames. The method achieves robust performance in both unconditional and conditional video generation tasks.

### Project 3: Cross-Modal Generative Framework
Designed a unified generative model capable of handling multiple modalities, such as text-to-image and image-to-video generation. The framework supports smooth cross-modal translation and demonstrates strong results on several public datasets.

---

*Feel free to reach out for research collaboration, academic discussions, or potential opportunities!*