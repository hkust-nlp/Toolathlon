import asyncio
from argparse import ArgumentParser
from pathlib import Path
from utils.general.helper import run_command, get_module_path
import os
import json
import logging
from datetime import datetime, date, timedelta
from google.cloud import bigquery
from google.oauth2 import service_account
from google.api_core.exceptions import Conflict, GoogleAPICallError, NotFound
import random
import numpy as np

# Enable verbose logging for debugging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def generate_player_skill_level():
    """ç”Ÿæˆç©å®¶æŠ€èƒ½ç­‰çº§ï¼Œæ¨¡æ‹ŸçœŸå®çš„æŠ€èƒ½åˆ†å¸ƒ"""
    # ä½¿ç”¨æ­£æ€åˆ†å¸ƒç”ŸæˆæŠ€èƒ½ç­‰çº§ï¼Œå¤§éƒ¨åˆ†ç©å®¶åœ¨ä¸­ç­‰æ°´å¹³
    skill = np.random.normal(50, 15)  # å¹³å‡50ï¼Œæ ‡å‡†å·®15
    return max(10, min(90, skill))  # é™åˆ¶åœ¨10-90ä¹‹é—´

def generate_realistic_score(base_skill, game_difficulty=1.0, variability=0.3):
    """åŸºäºæŠ€èƒ½ç­‰çº§ç”Ÿæˆæ›´çœŸå®çš„å¾—åˆ†"""
    # åŸºç¡€å¾—åˆ†åŸºäºæŠ€èƒ½
    base_score = base_skill * 10 + random.randint(-50, 50)
    
    # æ¸¸æˆéš¾åº¦å½±å“
    difficulty_modifier = random.uniform(0.8, 1.2) * game_difficulty
    
    # éšæœºæ³¢åŠ¨ï¼ˆæ¨¡æ‹Ÿè¿æ°”ã€çŠ¶æ€ç­‰å› ç´ ï¼‰
    random_factor = random.uniform(1 - variability, 1 + variability)
    
    final_score = int(base_score * difficulty_modifier * random_factor)
    return max(0, final_score)

def get_realistic_region_distribution():
    """è¿”å›æ›´çœŸå®çš„åœ°åŒºåˆ†å¸ƒæƒé‡"""
    regions = ["US", "EU", "ASIA", "CN"]
    weights = [0.3, 0.25, 0.25, 0.2]  # ç¾å›½ç¨å¤šï¼Œå…¶ä»–ç›¸å¯¹å‡åŒ€
    return random.choices(regions, weights=weights)[0]

def generate_game_timestamps(base_time, game_count):
    """ä¸ºä¸€ä¸ªç©å®¶çš„å¤šåœºæ¸¸æˆç”Ÿæˆä¸åŒçš„æ—¶é—´æˆ³"""
    timestamps = []
    current_time = base_time
    
    for i in range(game_count):
        # æ¯åœºæ¸¸æˆé—´éš”éšæœºæ—¶é—´ï¼ˆ1-60åˆ†é’Ÿï¼‰
        if i > 0:
            interval_minutes = random.randint(1, 60)
            current_time += timedelta(minutes=interval_minutes)
        timestamps.append(current_time)
    
    return timestamps

def generate_historical_stats_data(days_back=10, players_per_day=100):
    """ç”Ÿæˆå†å²ç»Ÿè®¡æ•°æ®ï¼ˆå‰Nå¤©çš„å‰100åç©å®¶æ•°æ®ï¼‰"""
    historical_data = []
    today = date.today()
    
    print(f"ğŸ“Š ç”Ÿæˆå†å²ç»Ÿè®¡æ•°æ®ï¼šè¿‡å» {days_back} å¤©ï¼Œæ¯å¤© {players_per_day} åç©å®¶")
    
    for day_offset in range(1, days_back + 1):
        target_date = today - timedelta(days=day_offset)
        print(f"   ç”Ÿæˆ {target_date} çš„æ•°æ®...")
        
        # ä¸ºè¿™ä¸€å¤©ç”Ÿæˆç©å®¶æ•°æ®
        day_players = []
        for rank in range(1, players_per_day + 1):
            # ç”Ÿæˆç©å®¶æŠ€èƒ½ç­‰çº§ï¼ˆæ’åè¶Šé å‰æŠ€èƒ½è¶Šé«˜ï¼‰
            base_skill = 95 - (rank - 1) * 0.5 + random.uniform(-5, 5)
            base_skill = max(20, min(95, base_skill))
            
            # ç”Ÿæˆè¯¥ç©å®¶å½“å¤©çš„æ€»å¾—åˆ†
            online_score = generate_realistic_score(base_skill, 1.0, 0.2) * random.randint(3, 8)
            task_score = generate_realistic_score(base_skill, 1.2, 0.25) * random.randint(2, 6)
            total_score = online_score + task_score
            
            # æ¸¸æˆåœºæ•°
            game_count = random.randint(3, 12)
            
            day_players.append({
                "player_id": f"player_{rank:03d}_{target_date.strftime('%m%d')}",
                "player_region": get_realistic_region_distribution(),
                "date": target_date.isoformat(),
                "total_online_score": online_score,
                "total_task_score": task_score,
                "total_score": total_score,
                "game_count": game_count
            })
        
        # æ ¹æ®æ€»åˆ†æ’åºï¼ˆç¡®ä¿æ’åæ­£ç¡®ï¼‰
        day_players.sort(key=lambda x: x["total_score"], reverse=True)
        historical_data.extend(day_players)
    
    print(f"âœ… ç”Ÿæˆäº† {len(historical_data)} æ¡å†å²ç»Ÿè®¡è®°å½•")
    return historical_data

def cleanup_existing_dataset(client: bigquery.Client, project_id: str):
    """
    Clean up existing game_analytics dataset if it exists
    """
    dataset_id = f"{project_id}.game_analytics"
    print(f"ğŸ§¹ æ£€æŸ¥å¹¶æ¸…ç†ç°æœ‰æ•°æ®é›†: {dataset_id}")
    
    try:
        # First try to get dataset info to see if it exists
        try:
            dataset = client.get_dataset(dataset_id)
            print(f"â„¹ï¸  æ‰¾åˆ°ç°æœ‰æ•°æ®é›†: {dataset_id}")
            
            # List all tables in the dataset
            tables = list(client.list_tables(dataset_id))
            if tables:
                print(f"â„¹ï¸  æ•°æ®é›†åŒ…å« {len(tables)} ä¸ªè¡¨:")
                for table in tables:
                    print(f"   - {table.table_id}")
        except NotFound:
            print(f"â„¹ï¸  æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
            return
        
        # Delete dataset with all contents
        print(f"ğŸ—‘ï¸  åˆ é™¤æ•°æ®é›†åŠå…¶æ‰€æœ‰å†…å®¹...")
        client.delete_dataset(
            dataset_id, 
            delete_contents=True, 
            not_found_ok=True
        )
        print(f"âœ… å·²æˆåŠŸæ¸…ç†æ•°æ®é›† '{dataset_id}' åŠå…¶æ‰€æœ‰å†…å®¹")
        
        # Wait a moment for deletion to propagate
        import time
        time.sleep(2)
        
    except NotFound:
        print(f"â„¹ï¸  æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")
        logger.exception("Dataset cleanup failed")
        raise

def setup_bigquery_resources(credentials_path: str, project_id: str):
    """
    Setup BigQuery dataset and tables, then populate with sample data
    """
    print("=" * 60)
    print("ğŸ¯ å¼€å§‹è®¾ç½® BigQuery æ¸¸æˆç»Ÿè®¡èµ„æº")
    print("=" * 60)
    
    try:
        print(f"ğŸ”— æ­£åœ¨ä½¿ç”¨å‡­è¯ '{credentials_path}' è¿æ¥åˆ°é¡¹ç›® '{project_id}'...")
        
        # Use the newer authentication method
        credentials = service_account.Credentials.from_service_account_file(credentials_path)
        client = bigquery.Client(credentials=credentials, project=project_id)
        
        print("âœ… è¿æ¥æˆåŠŸï¼")
        
        # Test connection by listing datasets
        print("ğŸ” æµ‹è¯•è¿æ¥ - åˆ—å‡ºç°æœ‰æ•°æ®é›†...")
        try:
            datasets = list(client.list_datasets())
            print(f"â„¹ï¸  é¡¹ç›®ä¸­ç°æœ‰ {len(datasets)} ä¸ªæ•°æ®é›†")
            for dataset in datasets:
                print(f"   - {dataset.dataset_id}")
        except Exception as e:
            print(f"âš ï¸  åˆ—å‡ºæ•°æ®é›†æ—¶å‡ºé”™: {e}")

        # Clean up existing dataset first
        cleanup_existing_dataset(client, project_id)

        # Create dataset
        dataset_id = f"{project_id}.game_analytics"
        print(f"ğŸ“Š åˆ›å»ºæ•°æ®é›†: {dataset_id}")
        try:
            dataset = bigquery.Dataset(dataset_id)
            dataset.location = "US"
            dataset.description = "Game analytics dataset for daily scoring and leaderboards"
            client.create_dataset(dataset, timeout=30)
            print(f"âœ… æ•°æ®é›† '{dataset.dataset_id}' å·²æˆåŠŸåˆ›å»ºã€‚")
        except Conflict:
            print(f"â„¹ï¸  æ•°æ®é›† '{dataset_id}' å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»ºã€‚")
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ•°æ®é›†å¤±è´¥: {e}")
            logger.exception("Dataset creation failed")
            raise

        # Create daily_scores_stream table
        table_id_stream = f"{dataset_id}.daily_scores_stream"
        print(f"ğŸ—‚ï¸  åˆ›å»ºè¡¨: {table_id_stream}")
        schema_stream = [
            bigquery.SchemaField("player_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("player_region", "STRING", mode="NULLABLE"),
            bigquery.SchemaField("scores", "RECORD", mode="NULLABLE", fields=[
                bigquery.SchemaField("online_score", "INTEGER"),
                bigquery.SchemaField("task_score", "INTEGER"),
            ]),
            bigquery.SchemaField("game_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("timestamp", "TIMESTAMP", mode="REQUIRED"),
        ]
        table_stream = bigquery.Table(table_id_stream, schema=schema_stream)
        table_stream.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field="timestamp",
        )
        try:
            client.create_table(table_stream)
            print(f"âœ… è¡¨ '{table_id_stream}' å·²æˆåŠŸåˆ›å»ºã€‚")
        except Conflict:
            print(f"â„¹ï¸  è¡¨ '{table_id_stream}' å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»ºã€‚")
        except Exception as e:
            print(f"âŒ åˆ›å»ºè¡¨ '{table_id_stream}' å¤±è´¥: {e}")
            raise

        # Create player_historical_stats table
        table_id_stats = f"{dataset_id}.player_historical_stats"
        print(f"ğŸ—‚ï¸  åˆ›å»ºè¡¨: {table_id_stats}")
        schema_stats = [
            bigquery.SchemaField("player_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("player_region", "STRING", mode="NULLABLE"),
            bigquery.SchemaField("date", "DATE", mode="REQUIRED"),
            bigquery.SchemaField("total_online_score", "INTEGER"),
            bigquery.SchemaField("total_task_score", "INTEGER"),
            bigquery.SchemaField("total_score", "INTEGER", description="å½“æ—¥æ€»åˆ† (online + task)"),
            bigquery.SchemaField("game_count", "INTEGER"),
        ]
        table_stats = bigquery.Table(table_id_stats, schema=schema_stats)
        try:
            client.create_table(table_stats)
            print(f"âœ… è¡¨ '{table_id_stats}' å·²æˆåŠŸåˆ›å»ºã€‚")
        except Conflict:
            print(f"â„¹ï¸  è¡¨ '{table_id_stats}' å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»ºã€‚")
        except Exception as e:
            print(f"âŒ åˆ›å»ºè¡¨ '{table_id_stats}' å¤±è´¥: {e}")
            raise

        # Populate with sample data for current date
        today = date.today()
        current_time = datetime.now()
        print(f"ğŸ“ˆ å¼€å§‹ç”Ÿæˆæ ·æœ¬æ•°æ®ï¼Œå½“å‰æ—¥æœŸ: {today}")
        
        # Generate improved sample data for daily_scores_stream
        sample_rows = []
        player_count = 200
        print(f"ğŸ‘¥ ä¸º {player_count} ä¸ªç©å®¶ç”Ÿæˆæ”¹è¿›çš„æ¸¸æˆæ•°æ®...")
        
        # ä¸ºæ¯ä¸ªç©å®¶é¢„å…ˆç”ŸæˆæŠ€èƒ½ç­‰çº§
        player_skills = {}
        for player_id in range(1, player_count + 1):
            player_skills[player_id] = generate_player_skill_level()
        
        for player_id in range(1, player_count + 1):
            # æ›´çœŸå®çš„æ¸¸æˆåœºæ¬¡åˆ†å¸ƒ
            games_count = random.choices([3, 4, 5, 6, 7, 8, 9, 10], 
                                       weights=[5, 10, 15, 20, 20, 15, 10, 5])[0]
            
            # ä¸ºè¯¥ç©å®¶ç”Ÿæˆæ¸¸æˆæ—¶é—´æˆ³
            start_time = current_time - timedelta(hours=random.randint(0, 12))
            timestamps = generate_game_timestamps(start_time, games_count)
            
            player_skill = player_skills[player_id]
            player_region = get_realistic_region_distribution()
            
            for game_num in range(games_count):
                # ä½¿ç”¨æ”¹è¿›çš„å¾—åˆ†ç”Ÿæˆå‡½æ•°
                game_difficulty = random.uniform(0.8, 1.5)  # éšæœºæ¸¸æˆéš¾åº¦
                online_score = generate_realistic_score(player_skill, game_difficulty, 0.3)
                task_score = generate_realistic_score(player_skill, game_difficulty * 1.2, 0.4)
                
                sample_rows.append({
                    "player_id": f"player_{player_id:03d}",
                    "player_region": player_region,
                    "scores": {
                        "online_score": online_score,
                        "task_score": task_score,
                    },
                    "game_id": f"game_{player_id:03d}_{game_num:02d}_{today.strftime('%Y%m%d')}",
                    "timestamp": timestamps[game_num].isoformat()
                })
        
        print(f"ğŸ“ ç”Ÿæˆäº† {len(sample_rows)} æ¡æ”¹è¿›çš„æ ·æœ¬è®°å½•")

        # Insert sample data with detailed error handling
        print(f"ğŸ’¾ æ’å…¥æ ·æœ¬æ•°æ®åˆ° daily_scores_stream...")
        try:
            table_ref = client.get_table(table_id_stream)
            print(f"âœ… è·å–åˆ°è¡¨å¼•ç”¨: {table_ref.table_id}")
            
            # Insert in smaller batches to avoid timeouts
            batch_size = 100
            total_inserted = 0
            for i in range(0, len(sample_rows), batch_size):
                batch = sample_rows[i:i + batch_size]
                print(f"   æ’å…¥æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} æ¡è®°å½•...")
                
                errors = client.insert_rows_json(table_ref, batch)
                if errors:
                    print(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥é”™è¯¯: {errors}")
                    for error in errors:
                        logger.error(f"Insert error: {error}")
                    raise Exception(f"æ‰¹æ¬¡æ’å…¥å¤±è´¥: {errors}")
                else:
                    total_inserted += len(batch)
                    print(f"   âœ… æ‰¹æ¬¡ {i//batch_size + 1} æˆåŠŸæ’å…¥ {len(batch)} æ¡è®°å½•")
            
            print(f"ğŸ‰ æˆåŠŸæ’å…¥æ€»è®¡ {total_inserted} æ¡æ ·æœ¬æ•°æ®åˆ° daily_scores_stream è¡¨")
            
            # Verify data insertion
            print("ğŸ” éªŒè¯æ•°æ®æ’å…¥...")
            query = f"""
            SELECT COUNT(*) as total_rows, 
                   COUNT(DISTINCT player_id) as unique_players,
                   DATE(timestamp) as data_date
            FROM `{table_id_stream}`
            WHERE DATE(timestamp) = '{today}'
            GROUP BY DATE(timestamp)
            """
            query_job = client.query(query)
            results = list(query_job.result())
            if results:
                result = results[0]
                print(f"âœ… éªŒè¯æˆåŠŸ: {result.total_rows} è¡Œæ•°æ®, {result.unique_players} ä¸ªç‹¬ç‰¹ç©å®¶, æ—¥æœŸ: {result.data_date}")
            else:
                print("âš ï¸  éªŒè¯æŸ¥è¯¢æœªè¿”å›ç»“æœ")
                
        except Exception as e:
            print(f"âŒ æ’å…¥æ•°æ®æ—¶å‡ºç°é”™è¯¯: {e}")
            logger.exception("Data insertion failed")
            raise Exception(f"æ’å…¥æ•°æ®å¤±è´¥: {e}")

        # Generate and insert historical data for player_historical_stats
        print(f"\nğŸ“ˆ å¼€å§‹ç”Ÿæˆå¹¶æ’å…¥å†å²ç»Ÿè®¡æ•°æ®...")
        try:
            historical_data = generate_historical_stats_data(days_back=10, players_per_day=100)
            
            # Insert historical data
            table_ref_stats = client.get_table(table_id_stats)
            print(f"ğŸ’¾ æ’å…¥å†å²ç»Ÿè®¡æ•°æ®åˆ° player_historical_stats...")
            
            # Insert in batches
            batch_size = 100
            total_inserted = 0
            for i in range(0, len(historical_data), batch_size):
                batch = historical_data[i:i + batch_size]
                print(f"   æ’å…¥å†å²æ•°æ®æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} æ¡è®°å½•...")
                
                errors = client.insert_rows_json(table_ref_stats, batch)
                if errors:
                    print(f"âŒ å†å²æ•°æ®æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥é”™è¯¯: {errors}")
                    for error in errors:
                        logger.error(f"Historical data insert error: {error}")
                    raise Exception(f"å†å²æ•°æ®æ‰¹æ¬¡æ’å…¥å¤±è´¥: {errors}")
                else:
                    total_inserted += len(batch)
                    print(f"   âœ… å†å²æ•°æ®æ‰¹æ¬¡ {i//batch_size + 1} æˆåŠŸæ’å…¥ {len(batch)} æ¡è®°å½•")
            
            print(f"ğŸ‰ æˆåŠŸæ’å…¥æ€»è®¡ {total_inserted} æ¡å†å²ç»Ÿè®¡æ•°æ®åˆ° player_historical_stats è¡¨")
            
            # Verify historical data insertion
            print("ğŸ” éªŒè¯å†å²æ•°æ®æ’å…¥...")
            query = f"""
            SELECT COUNT(*) as total_rows, 
                   COUNT(DISTINCT date) as unique_dates,
                   MIN(date) as earliest_date,
                   MAX(date) as latest_date
            FROM `{table_id_stats}`
            """
            query_job = client.query(query)
            results = list(query_job.result())
            if results:
                result = results[0]
                print(f"âœ… å†å²æ•°æ®éªŒè¯æˆåŠŸ: {result.total_rows} è¡Œæ•°æ®, {result.unique_dates} ä¸ªä¸åŒæ—¥æœŸ")
                print(f"   æ—¥æœŸèŒƒå›´: {result.earliest_date} åˆ° {result.latest_date}")
            else:
                print("âš ï¸  å†å²æ•°æ®éªŒè¯æŸ¥è¯¢æœªè¿”å›ç»“æœ")
                
        except Exception as e:
            print(f"âŒ æ’å…¥å†å²æ•°æ®æ—¶å‡ºç°é”™è¯¯: {e}")
            logger.exception("Historical data insertion failed")
            raise Exception(f"æ’å…¥å†å²æ•°æ®å¤±è´¥: {e}")

        return client, dataset_id

    except GoogleAPICallError as e:
        print(f"âŒ Google Cloud API è°ƒç”¨å¤±è´¥: {e}")
        logger.exception("Google Cloud API call failed")
        raise
    except Exception as e:
        print(f"âŒ è®¾ç½®è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.exception("Setup process failed")
        raise

def get_project_id_from_key(credentials_path: str) -> str | None:
    """ä»æœåŠ¡è´¦å·å¯†é’¥æ–‡ä»¶ä¸­è¯»å–é¡¹ç›®ID"""
    try:
        with open(credentials_path, 'r') as f:
            data = json.load(f)
            return data.get("project_id")
    except (FileNotFoundError, json.JSONDecodeError):
        return None

if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("--agent_workspace", required=False)
    parser.add_argument("--credentials_file", default="configs/mcp-bench0606-2b68b5487343.json")
    parser.add_argument("--launch_time", nargs='*', required=False, help="Launch time (can contain spaces)")
    args = parser.parse_args()

    print("ğŸ® å¼€å§‹è®¾ç½® BigQuery æ¸¸æˆç»Ÿè®¡èµ„æº...")
    print("=" * 60)
    
    # Get credentials file path
    credentials_path = Path(args.credentials_file)
    
    # Make sure the path is absolute
    if not credentials_path.is_absolute():
        credentials_path = Path.cwd() / credentials_path
    
    if not credentials_path.exists():
        print(f"âŒ é”™è¯¯ï¼šå‡­è¯æ–‡ä»¶ä¸å­˜åœ¨: {credentials_path}")
        print("è¯·ç¡®ä¿æœåŠ¡è´¦å·å¯†é’¥æ–‡ä»¶å­˜åœ¨äºæŒ‡å®šè·¯å¾„")
        exit(1)
    else:
        print(f"âœ… æ‰¾åˆ°å‡­è¯æ–‡ä»¶: {credentials_path}")
    
    project_id = get_project_id_from_key(str(credentials_path))
    
    if project_id:
        print(f"ğŸ†” ä»å‡­è¯æ–‡ä»¶ä¸­æˆåŠŸè¯»å–é¡¹ç›®ID: {project_id}")
        try:
            client, dataset_id = setup_bigquery_resources(str(credentials_path), project_id)
            print("\n" + "=" * 60)
            print("ğŸ‰ æ‰€æœ‰ BigQuery èµ„æºè®¾ç½®å®Œæ¯•ï¼")
            print("ğŸ“Š å·²ä¸ºå½“æ—¥ç”Ÿæˆæ ·æœ¬æ¸¸æˆæ•°æ®")
            print("ğŸ¯ ä»»åŠ¡ï¼šä»£ç†éœ€è¦ç”Ÿæˆæ’è¡Œæ¦œå¹¶æ›´æ–°å†å²ç»Ÿè®¡æ•°æ®")
            print("=" * 60)
        except Exception as e:
            print(f"\nâŒ è®¾ç½®å¤±è´¥: {e}")
            exit(1)
    else:
        print(f"âŒ æ— æ³•ä»å‡­è¯æ–‡ä»¶ä¸­è¯»å–é¡¹ç›®IDã€‚")
        exit(1)