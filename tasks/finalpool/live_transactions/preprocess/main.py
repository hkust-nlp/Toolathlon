from argparse import ArgumentParser
import os
import subprocess
import json

def check_gcloud_authentication():
    """æ£€æŸ¥Google Cloud CLIæ˜¯å¦å·²è®¤è¯"""
    try:
        result = subprocess.run(['gcloud', 'auth', 'list', '--filter=status:ACTIVE', '--format=value(account)'], 
                              capture_output=True, text=True, check=True)
        if '@' in result.stdout:
            return True
        return False
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

def delete_investigation_file_if_exists(bucket_name="mcp-fraud-investigation-archive", file_name="T8492XJ3.json", project_id="mcp-bench0606"):
    """åˆ é™¤Google Cloud Storageå­˜å‚¨æ¡¶ä¸­çš„è°ƒæŸ¥æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    print(f"ğŸ§¹ Checking for existing investigation file: gs://{bucket_name}/{file_name}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        check_result = subprocess.run(['gcloud', 'storage', 'ls', f'gs://{bucket_name}/{file_name}'], 
                                    capture_output=True, text=True)
        
        if check_result.returncode == 0:
            print(f"ğŸ“„ Found existing investigation file: {file_name}")
            print(f"ğŸ—‘ï¸  Deleting file: {file_name} to allow fresh task execution...")
            
            # åˆ é™¤æ–‡ä»¶
            delete_result = subprocess.run(['gcloud', 'storage', 'rm', f'gs://{bucket_name}/{file_name}'], 
                                         capture_output=True, text=True)
            
            if delete_result.returncode == 0:
                print(f"âœ… Successfully deleted investigation file: {file_name}")
                return True
            else:
                print(f"âŒ Failed to delete investigation file: {file_name}")
                print(f"Error: {delete_result.stderr}")
                return False
        else:
            print(f"âœ… Investigation file {file_name} does not exist - no cleanup needed")
            return True
            
    except FileNotFoundError:
        print("âŒ Error: gcloud command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error checking/deleting investigation file: {e}")
        return False

def ensure_bucket_exists(bucket_name="mcp-fraud-investigation-archive", project_id="mcp-bench0606"):
    """ç¡®ä¿å­˜å‚¨æ¡¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    print(f"ğŸ” Checking if bucket exists: {bucket_name}")
    
    try:
        # æ£€æŸ¥å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨
        check_result = subprocess.run(['gcloud', 'storage', 'ls', f'gs://{bucket_name}'], 
                                    capture_output=True, text=True)
        
        if check_result.returncode == 0:
            print(f"âœ… Bucket {bucket_name} already exists")
            return True
        else:
            print(f"ğŸ“¦ Creating bucket: {bucket_name}")
            # åˆ›å»ºå­˜å‚¨æ¡¶
            create_result = subprocess.run(['gcloud', 'storage', 'buckets', 'create', f'gs://{bucket_name}', 
                                          '--project', project_id, '--location=us-central1'], 
                                         capture_output=True, text=True)
            
            if create_result.returncode == 0:
                print(f"âœ… Successfully created bucket: {bucket_name}")
                return True
            else:
                print(f"âŒ Failed to create bucket: {bucket_name}")
                print(f"Error: {create_result.stderr}")
                return False
                
    except FileNotFoundError:
        print("âŒ Error: gcloud command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error checking/creating bucket: {e}")
        return False

def check_log_bucket_exists(bucket_name="Trading_Logging", project_id="mcp-bench0606"):
    """æ£€æŸ¥Google Cloud Logging bucketæ˜¯å¦å­˜åœ¨"""
    print(f"ğŸ” Checking if log bucket exists: {bucket_name}")
    
    try:
        # æ£€æŸ¥log bucketæ˜¯å¦å­˜åœ¨ - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼è·å–bucketåç§°
        check_result = subprocess.run(['gcloud', 'logging', 'buckets', 'list', 
                                     '--project', project_id, '--format=value(name.basename())'], 
                                    capture_output=True, text=True)
        
        if check_result.returncode == 0:
            # æ£€æŸ¥è¾“å‡ºä¸­æ˜¯å¦åŒ…å«æˆ‘ä»¬çš„bucketåç§°
            buckets = check_result.stdout.strip().split('\n')
            for bucket in buckets:
                if bucket.strip() == bucket_name:
                    print(f"âœ… Log bucket {bucket_name} already exists")
                    return True
            
            print(f"ğŸ“ Log bucket {bucket_name} does not exist")
            return False
        else:
            print(f"âŒ Failed to list log buckets")
            print(f"Error: {check_result.stderr}")
            return False
            
    except FileNotFoundError:
        print("âŒ Error: gcloud command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error checking log bucket: {e}")
        return False

def create_log_bucket(bucket_name="Trading_Logging", project_id="mcp-bench0606", location="global"):
    """åˆ›å»ºGoogle Cloud Logging bucket"""
    print(f"ğŸ“ Creating log bucket: {bucket_name}")
    
    try:
        # åˆ›å»ºlog bucket
        create_result = subprocess.run(['gcloud', 'logging', 'buckets', 'create', bucket_name,
                                      '--project', project_id, '--location', location,
                                      '--retention-days=30'], 
                                     capture_output=True, text=True)
        
        if create_result.returncode == 0:
            print(f"âœ… Successfully created log bucket: {bucket_name}")
            return True
        else:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºbucketå·²å­˜åœ¨è€Œå¤±è´¥
            if "ALREADY_EXISTS" in create_result.stderr:
                print(f"âœ… Log bucket {bucket_name} already exists (detected during creation)")
                return True
            else:
                print(f"âŒ Failed to create log bucket: {bucket_name}")
                print(f"Error: {create_result.stderr}")
                return False
            
    except FileNotFoundError:
        print("âŒ Error: gcloud command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error creating log bucket: {e}")
        return False

def clear_log_bucket_logs(bucket_name="Trading_Logging", project_id="mcp-bench0606"):
    """æ¸…ç©ºlog bucketä¸­çš„æ—¥å¿—"""
    print(f"ğŸ§¹ Clearing logs from log bucket: {bucket_name}")
    
    try:
        # é¦–å…ˆåˆ—å‡ºbucketä¸­çš„æ—¥å¿—
        list_result = subprocess.run(['gcloud', 'logging', 'logs', 'list', 
                                    '--project', project_id, '--format=value(name)'], 
                                   capture_output=True, text=True)
        
        if list_result.returncode != 0:
            print(f"âŒ Failed to list logs in project")
            print(f"Error: {list_result.stderr}")
            return False
        
        # åˆ é™¤ä¸Tradingç›¸å…³çš„æ—¥å¿—
        logs_to_delete = []
        if list_result.stdout.strip():
            all_logs = list_result.stdout.strip().split('\n')
            logs_to_delete = [log for log in all_logs if 'trading' in log.lower() or 'transaction' in log.lower()]
        
        if logs_to_delete:
            print(f"ğŸ—‘ï¸  Found {len(logs_to_delete)} trading-related logs to clear")
            
            for log_name in logs_to_delete:
                delete_result = subprocess.run(['gcloud', 'logging', 'logs', 'delete', log_name,
                                              '--project', project_id, '--quiet'], 
                                             capture_output=True, text=True)
                
                if delete_result.returncode == 0:
                    print(f"âœ… Cleared log: {log_name}")
                else:
                    print(f"âš ï¸  Failed to clear log: {log_name}")
        else:
            print(f"âœ… No trading-related logs found to clear")
        
        return True
        
    except FileNotFoundError:
        print("âŒ Error: gcloud command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error clearing log bucket logs: {e}")
        return False

def manage_trading_log_bucket(project_id="mcp-bench0606", bucket_name="Trading_Logging"):
    """ç®¡ç†Trading_Logging log bucketçš„å®Œæ•´æµç¨‹"""
    print(f"ğŸ“Š Managing Trading_Logging log bucket...")
    
    results = {
        "bucket_exists": False,
        "bucket_created": False,
        "logs_cleared": False
    }
    
    # æ£€æŸ¥bucketæ˜¯å¦å­˜åœ¨
    bucket_exists = check_log_bucket_exists(bucket_name, project_id)
    results["bucket_exists"] = bucket_exists
    
    if bucket_exists:
        # å¦‚æœå­˜åœ¨ï¼Œæ¸…ç©ºæ—¥å¿—
        logs_cleared = clear_log_bucket_logs(bucket_name, project_id)
        results["logs_cleared"] = logs_cleared
        print(f"âœ… Log bucket {bucket_name} is ready (logs cleared)")
    else:
        # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„bucket
        bucket_created = create_log_bucket(bucket_name, project_id)
        results["bucket_created"] = bucket_created
        if bucket_created:
            print(f"âœ… Log bucket {bucket_name} is ready (newly created)")
        else:
            print(f"âŒ Failed to prepare log bucket {bucket_name}")
    
    return results

def check_bq_dataset_exists(dataset_name="transactions_analytics", project_id="mcp-bench0606"):
    """æ£€æŸ¥BigQueryæ•°æ®é›†æ˜¯å¦å­˜åœ¨"""
    print(f"ğŸ” Checking if BigQuery dataset exists: {dataset_name}")
    
    try:
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨ - ä½¿ç”¨ç®€å•çš„åˆ—è¡¨å‘½ä»¤ç„¶åè§£æ
        check_result = subprocess.run(['bq', 'ls', '--project_id', project_id], 
                                    capture_output=True, text=True)
        
        if check_result.returncode == 0:
            # è§£æè¾“å‡ºï¼Œè·³è¿‡è¡¨å¤´ï¼Œæå–ç¬¬ä¸€åˆ—ï¼ˆæ•°æ®é›†åç§°ï¼‰
            lines = check_result.stdout.strip().split('\n')
            if len(lines) > 2:  # è·³è¿‡è¡¨å¤´
                for line in lines[2:]:  # ä»ç¬¬3è¡Œå¼€å§‹
                    if line.strip():
                        dataset = line.strip().split()[0]  # è·å–ç¬¬ä¸€åˆ—
                        if dataset == dataset_name:
                            print(f"âœ… BigQuery dataset {dataset_name} already exists")
                            return True
            
            print(f"ğŸ“Š BigQuery dataset {dataset_name} does not exist")
            return False
        else:
            print(f"âŒ Failed to list BigQuery datasets")
            print(f"Error: {check_result.stderr}")
            return False
            
    except FileNotFoundError:
        print("âŒ Error: bq command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error checking BigQuery dataset: {e}")
        return False

def delete_bq_dataset(dataset_name="transactions_analytics", project_id="mcp-bench0606"):
    """åˆ é™¤BigQueryæ•°æ®é›†"""
    print(f"ğŸ—‘ï¸  Deleting BigQuery dataset: {dataset_name}")
    
    try:
        # åˆ é™¤æ•°æ®é›†ï¼ˆåŒ…æ‹¬æ‰€æœ‰è¡¨ï¼‰
        delete_result = subprocess.run(['bq', 'rm', '-r', '-f', '--project_id', project_id, dataset_name], 
                                     capture_output=True, text=True)
        
        if delete_result.returncode == 0:
            print(f"âœ… Successfully deleted BigQuery dataset: {dataset_name}")
            return True
        else:
            print(f"âŒ Failed to delete BigQuery dataset: {dataset_name}")
            print(f"Error: {delete_result.stderr}")
            return False
            
    except FileNotFoundError:
        print("âŒ Error: bq command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error deleting BigQuery dataset: {e}")
        return False

def create_bq_dataset(dataset_name="transactions_analytics", project_id="mcp-bench0606", location="US"):
    """åˆ›å»ºBigQueryæ•°æ®é›†"""
    print(f"ğŸ“Š Creating BigQuery dataset: {dataset_name}")
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        create_result = subprocess.run(['bq', 'mk', '--project_id', project_id, '--location', location, dataset_name], 
                                     capture_output=True, text=True)
        
        if create_result.returncode == 0:
            print(f"âœ… Successfully created BigQuery dataset: {dataset_name}")
            return True
        else:
            print(f"âŒ Failed to create BigQuery dataset: {dataset_name}")
            print(f"Error: {create_result.stderr}")
            return False
            
    except FileNotFoundError:
        print("âŒ Error: bq command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error creating BigQuery dataset: {e}")
        return False

def upload_csv_to_bq_table(csv_file_path, table_name, dataset_name="transactions_analytics", project_id="mcp-bench0606"):
    """ä¸Šä¼ CSVæ–‡ä»¶åˆ°BigQueryè¡¨"""
    print(f"ğŸ“¤ Uploading {os.path.basename(csv_file_path)} to BigQuery table: {table_name}")
    
    try:
        # æ„å»ºè¡¨çš„å®Œæ•´è·¯å¾„
        table_path = f"{project_id}:{dataset_name}.{table_name}"
        
        # ä¸Šä¼ CSVæ–‡ä»¶åˆ°BigQueryè¡¨ï¼Œè‡ªåŠ¨æ£€æµ‹schema
        upload_result = subprocess.run(['bq', 'load', '--autodetect', '--source_format=CSV', 
                                      '--replace', table_path, csv_file_path], 
                                     capture_output=True, text=True)
        
        if upload_result.returncode == 0:
            print(f"âœ… Successfully uploaded {os.path.basename(csv_file_path)} to table: {table_name}")
            return True
        else:
            print(f"âŒ Failed to upload {os.path.basename(csv_file_path)} to table: {table_name}")
            print(f"Error: {upload_result.stderr}")
            return False
            
    except FileNotFoundError:
        print("âŒ Error: bq command not found. Please install Google Cloud SDK.")
        return False
    except Exception as e:
        print(f"âŒ Error uploading CSV to BigQuery table: {e}")
        return False

def manage_transactions_analytics_dataset(project_id="mcp-bench0606", dataset_name="transactions_analytics", csv_directory="transactions_analytics"):
    """ç®¡ç†transactions_analytics BigQueryæ•°æ®é›†çš„å®Œæ•´æµç¨‹"""
    print(f"ğŸ“Š Managing BigQuery dataset: {dataset_name}")
    
    results = {
        "dataset_existed": False,
        "dataset_deleted": False,
        "dataset_created": False,
        "tables_uploaded": [],
        "upload_failures": []
    }
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    dataset_exists = check_bq_dataset_exists(dataset_name, project_id)
    results["dataset_existed"] = dataset_exists
    
    if dataset_exists:
        # å¦‚æœå­˜åœ¨ï¼Œåˆ é™¤æ•°æ®é›†
        dataset_deleted = delete_bq_dataset(dataset_name, project_id)
        results["dataset_deleted"] = dataset_deleted
        if not dataset_deleted:
            print(f"âŒ Failed to delete existing dataset {dataset_name}")
            return results
    
    # åˆ›å»ºæ–°çš„æ•°æ®é›†
    dataset_created = create_bq_dataset(dataset_name, project_id)
    results["dataset_created"] = dataset_created
    
    if not dataset_created:
        print(f"âŒ Failed to create dataset {dataset_name}")
        return results
    
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_dir_path = os.path.join(script_dir, csv_directory)
    
    if not os.path.exists(csv_dir_path):
        print(f"âŒ CSV directory not found: {csv_dir_path}")
        return results
    
    # ä¸Šä¼ æ‰€æœ‰CSVæ–‡ä»¶åˆ°BigQueryè¡¨
    csv_files = [f for f in os.listdir(csv_dir_path) if f.endswith('.csv')]
    print(f"ğŸ“ Found {len(csv_files)} CSV files to upload")
    
    for csv_file in csv_files:
        csv_file_path = os.path.join(csv_dir_path, csv_file)
        table_name = os.path.splitext(csv_file)[0]  # ç§»é™¤.csvåç¼€ä½œä¸ºè¡¨å
        
        upload_success = upload_csv_to_bq_table(csv_file_path, table_name, dataset_name, project_id)
        
        if upload_success:
            results["tables_uploaded"].append(table_name)
        else:
            results["upload_failures"].append(csv_file)
    
    print(f"âœ… Dataset {dataset_name} management completed:")
    print(f"   - Tables uploaded: {len(results['tables_uploaded'])}")
    print(f"   - Upload failures: {len(results['upload_failures'])}")
    
    return results

def cleanup_preprocess_environment(workspace_dir, target_transaction_id="T8492XJ3", project_id="mcp-bench0606"):
    """æ¸…ç†preprocessç¯å¢ƒï¼Œä¸ºLive Transactionsä»»åŠ¡åšå‡†å¤‡"""
    print("ğŸš€ Starting Live Transactions Preprocess Cleanup...")
    
    # æ£€æŸ¥Google Cloudè®¤è¯
    if not check_gcloud_authentication():
        print("âš ï¸  Warning: Google Cloud CLI not authenticated. Some cleanup may fail.")
        print("   Please run: gcloud auth login")
    
    cleanup_results = {}
    
    # ç¡®ä¿å­˜å‚¨æ¡¶å­˜åœ¨
    bucket_ready = ensure_bucket_exists("mcp-fraud-investigation-archive", project_id)
    cleanup_results["bucket_ready"] = bucket_ready
    
    # åˆ é™¤ç›®æ ‡äº¤æ˜“çš„è°ƒæŸ¥æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    file_cleanup = delete_investigation_file_if_exists("mcp-fraud-investigation-archive", f"{target_transaction_id}.json", project_id)
    cleanup_results["file_cleanup"] = file_cleanup
    
    # ç®¡ç†Trading_Logging log bucket
    log_bucket_results = manage_trading_log_bucket(project_id)
    cleanup_results["log_bucket_results"] = log_bucket_results

    # ç®¡ç†transactions_analytics BigQueryæ•°æ®é›†
    bq_dataset_results = manage_transactions_analytics_dataset(project_id)
    cleanup_results["bq_dataset_results"] = bq_dataset_results
    
    # ä¿å­˜æ¸…ç†ç»“æœåˆ°workspace
    results_file = os.path.join(workspace_dir, "preprocess_cleanup_results.json")
    with open(results_file, 'w') as f:
        json.dump({
            "cleanup_timestamp": subprocess.run(['date', '+%Y-%m-%d %H:%M:%S'], 
                                              capture_output=True, text=True).stdout.strip(),
            "target_transaction_id": target_transaction_id,
            "bucket_ready": bucket_ready,
            "file_cleanup_success": file_cleanup,
            "log_bucket_results": log_bucket_results,
            "bq_dataset_results": bq_dataset_results,
            "cleaned_files": [f"{target_transaction_id}.json"] if file_cleanup else [],
            "bucket_name": "mcp-fraud-investigation-archive",
            "status": "completed"
        }, f, indent=2)
    
    print(f"ğŸ“‹ Cleanup results saved to: {results_file}")
    return cleanup_results



if __name__=="__main__":
    parser = ArgumentParser()
    parser.add_argument("--agent_workspace", required=True)
    parser.add_argument("--transaction_id", default="T8492XJ3",
                       help="Target suspicious transaction ID (default: T8492XJ3)")
    parser.add_argument("--cleanup_files", default=True, action="store_true",
                       help="Clean up existing investigation files to prevent task interference (default: enabled)")
    parser.add_argument("--no_cleanup", action="store_true",
                       help="Skip file cleanup (use when you want to preserve existing files)")
    parser.add_argument("--project_id", default="mcp-bench0606",
                       help="Google Cloud Project ID")
    parser.add_argument("--launch_time", required=False, help="Launch time")
    
    args = parser.parse_args()
    
    print("=== Live Transactions Fraud Investigation Preprocess ===")
    print(f"Agent workspace: {args.agent_workspace}")
    print(f"Target transaction ID: {args.transaction_id}")
    print(f"Project ID: {args.project_id}")
    
    # ç¡®ä¿workspaceç›®å½•å­˜åœ¨
    os.makedirs(args.agent_workspace, exist_ok=True)
    
    # é»˜è®¤æ‰§è¡Œæ¸…ç†ï¼Œé™¤éæ˜ç¡®æŒ‡å®šä¸æ¸…ç†
    should_cleanup = args.cleanup_files and not args.no_cleanup
    
    if should_cleanup:
        print(f"\nğŸ§¹ Performing cleanup for transaction {args.transaction_id}...")
        cleanup_results = cleanup_preprocess_environment(args.agent_workspace, args.transaction_id, args.project_id)
        
        if cleanup_results.get("file_cleanup", False) or cleanup_results.get("bucket_ready", False) or cleanup_results.get("log_bucket_results", {}).get("bucket_exists", False) or cleanup_results.get("log_bucket_results", {}).get("bucket_created", False) or cleanup_results.get("bq_dataset_results", {}).get("dataset_existed", False) or cleanup_results.get("bq_dataset_results", {}).get("dataset_deleted", False) or cleanup_results.get("bq_dataset_results", {}).get("dataset_created", False) or cleanup_results.get("bq_dataset_results", {}).get("tables_uploaded"):
            print("âœ… Preprocess cleanup completed successfully!")
        else:
            print("âš ï¸  Preprocess cleanup completed with warnings.")
    else:
        print("â„¹ï¸  Cleanup skipped (--no_cleanup specified).")
    
    # åˆ›å»ºæœ€ç»ˆç»“æœæ–‡ä»¶
    results_file = os.path.join(args.agent_workspace, "preprocess_results.json")
    final_results = {
        "timestamp": subprocess.run(['date', '+%Y-%m-%d %H:%M:%S'], 
                                  capture_output=True, text=True).stdout.strip(),
        "target_transaction_id": args.transaction_id,
        "cleanup_performed": should_cleanup,
        "bucket_name": "mcp-fraud-investigation-archive",
        "log_bucket_name": "Trading_Logging",
        "bq_dataset_name": "transactions_analytics",
        "expected_output_file": f"{args.transaction_id}.json",
        "status": "ready_for_investigation",
        "task_description": "Investigate suspicious transaction and upload to archive bucket"
    }
    
    # å¦‚æœæ‰§è¡Œäº†æ¸…ç†ï¼Œæ·»åŠ è¯¦ç»†ç»“æœ
    if should_cleanup and 'cleanup_results' in locals():
        final_results["cleanup_details"] = cleanup_results
        
    with open(results_file, 'w') as f:
        json.dump(final_results, f, indent=2)
    
    print(f"\nğŸ¯ Environment prepared for fraud investigation!")
    print(f"ğŸ” Ready to investigate transaction: {args.transaction_id}")
    print(f"ğŸ“¤ Target upload location: gs://mcp-fraud-investigation-archive/{args.transaction_id}.json")
    print(f"ğŸ“Š Trading_Logging log bucket is ready for transaction logging")
    print(f"ğŸ’¾ BigQuery transactions_analytics dataset is ready with fresh data")
    print("ğŸš¨ Ready for task execution - environment has been prepared and cleaned up")
