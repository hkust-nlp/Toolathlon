#!/usr/bin/env python3
"""
Evaluation script for GitHub milestone repository information collection task.
This script checks if the collected repository information matches the expected data
and validates that repo ID 1000 is properly handled (should be missing or marked as unknown).
"""

import json
import os
import sys
from typing import Dict, Any, List, Optional
from argparse import ArgumentParser

# Import normalize_str from utils
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
from utils.general.helper import normalize_str

# Add task utils to path for GitHub fetcher
sys.path.append(os.path.dirname(__file__))
from utils.github_fetcher import fetch_and_save_github_data

class GitHubRepoEvaluator:
    def __init__(self, agent_file_path: str, before_task_file_path: str = None, after_task_file_path: str = None):
        self.agent_file_path = agent_file_path
        self.before_task_file_path = before_task_file_path
        self.after_task_file_path = after_task_file_path
        self.expected_repo_ids = [1, 1000, 1000000, 1000000000]
        self.required_fields = [
            'repo_name', 'owner', 'star_count', 'fork_count', 
            'creation_date', 'description', 'language', 'repo_url'
        ]
        
        # Load before and after task data
        self.before_task_data = {}
        self.after_task_data = {}
        
        if before_task_file_path:
            self.before_task_data = self.load_groundtruth_data(before_task_file_path, "before_task")
        
        if after_task_file_path:
            self.after_task_data = self.load_groundtruth_data(after_task_file_path, "after_task")
        
        # If no dynamic data available, use fallback
        if not self.before_task_data and not self.after_task_data:
            # Fallback to hardcoded data if no groundtruth files
            self.before_task_data = {
                "1": {
                    "repo_name": "grit",
                    "owner": "mojombo",
                    "star_count": 1900,
                    "fork_count": 500,
                    "creation_date": "2007-10-29T14:37:16Z",
                    "description": "**Grit is no longer maintained. Check out libgit2/rugged.** Grit gives you object oriented read/write access to Git repositories via Ruby.",
                    "language": "Ruby",
                    "repo_url": "https://github.com/mojombo/grit"
                },
                "1000000": {
                    "repo_name": "nexus.vim",
                    "owner": "vim-scripts",
                    "star_count": 0,
                    "fork_count": 0,
                    "creation_date": "2010-10-18T18:52:14Z",
                    "description": "Syntax highlighting for Nexus file format",
                    "language": "VimL",
                    "repo_url": "https://github.com/vim-scripts/nexus.vim"
                },
                "1000000000": {
                    "repo_name": "shit",
                    "owner": "Red-Killer",
                    "star_count": 3600,
                    "fork_count": 260,
                    "creation_date": "2025-06-11T05:50:39Z",
                    "description": None,
                    "language": None,
                    "repo_url": "https://github.com/Red-Killer/shit"
                }
            }
            self.after_task_data = self.before_task_data.copy()
    
    def load_groundtruth_data(self, file_path: str, data_type: str) -> Dict[str, Any]:
        """Load groundtruth JSON data from file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ Loaded {data_type} data from {file_path}")
                return data
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Failed to load {data_type} data from {file_path}: {e}")
            return {}
    
    def load_data(self) -> Optional[Dict[str, Any]]:
        """Load JSON data from agent file."""
        try:
            with open(self.agent_file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"‚ùå Error: File {self.agent_file_path} not found")
            return None
        except json.JSONDecodeError as e:
            print(f"‚ùå Error: Invalid JSON format in {self.agent_file_path}: {e}")
            return None
        except Exception as e:
            print(f"‚ùå Error: Failed to load {self.agent_file_path}: {e}")
            return None
    
    def generate_after_task_data(self) -> bool:
        """Generate after_task.json with current GitHub stats."""
        if not self.after_task_file_path:
            print("‚ö†Ô∏è  No after_task file path specified, skipping generation")
            return True
            
        print("üìä Generating after_task.json with current GitHub stats...")
        try:
            # Fetch current GitHub data
            repo_data = fetch_and_save_github_data(self.after_task_file_path, verbose=False)
            if repo_data:
                print(f"‚úÖ Generated after_task.json with {len(repo_data)} repositories")
                # Reload the after_task_data for evaluation
                self.after_task_data = self.load_groundtruth_data(self.after_task_file_path, "after_task")
                return True
            else:
                print("‚ùå Failed to fetch current GitHub data for after_task.json")
                return False
        except Exception as e:
            print(f"‚ùå Error generating after_task.json: {e}")
            return False
    
    def check_file_format(self, data: Dict[str, Any]) -> bool:
        """Check if the file has the correct JSON format."""
        if not isinstance(data, dict):
            print("‚ùå Error: Root element should be a dictionary")
            return False
        
        print("‚úÖ File format: Valid JSON dictionary")
        return True
    
    def check_repo_1000_handling(self, data: Dict[str, Any]) -> bool:
        """Check if repo ID 1000 is properly handled (missing or marked as unknown)."""
        repo_1000_key = "1000"
        
        # If before/after task data contains repo 1000, validate it normally
        if "1000" in self.before_task_data or "1000" in self.after_task_data:
            if repo_1000_key in data:
                print("‚úÖ Repo ID 1000: Present in results (as expected from groundtruth)")
                return True
            else:
                print("‚ùå Repo ID 1000: Missing from results but expected from groundtruth")
                return False
        
        # Original logic for when groundtruth doesn't contain repo 1000
        if repo_1000_key not in data:
            print("‚úÖ Repo ID 1000: Correctly excluded from results")
            return True
        
        # Check if it's marked as unknown/unavailable
        repo_1000_data = data[repo_1000_key]
        if isinstance(repo_1000_data, dict):
            # Check if it's marked as unknown or has placeholder values
            unknown_indicators = [
                repo_1000_data.get('repo_name') in ['unknown', 'unavailable', None, ''],
                repo_1000_data.get('owner') in ['unknown', 'unavailable', None, ''],
                'unknown' in str(repo_1000_data).lower(),
                'unavailable' in str(repo_1000_data).lower(),
                'not found' in str(repo_1000_data).lower()
            ]
            
            if any(unknown_indicators):
                print("‚úÖ Repo ID 1000: Correctly marked as unknown/unavailable")
                return True
        
        print("‚ùå Repo ID 1000: Should be excluded or marked as unknown")
        return False
    
    def validate_repo_data(self, repo_id: str, actual_data: Dict[str, Any], before_data: Dict[str, Any], after_data: Dict[str, Any]) -> bool:
        """Validate a single repository's data against before/after task snapshots."""
        success = True
        
        # Check required fields
        for field in self.required_fields:
            if field not in actual_data:
                print(f"‚ùå Repo {repo_id}: Missing required field '{field}'")
                success = False
        
        # Get reference data (prefer after_data, fallback to before_data)
        reference_data = after_data if after_data else before_data
        if not reference_data:
            print(f"‚ö†Ô∏è  Repo {repo_id}: No reference data available, skipping validation")
            return True
        
        # Check data accuracy with range validation
        for field, reference_value in reference_data.items():
            if field in actual_data:
                actual_value = actual_data[field]
                
                # Special handling for numeric fields that can change over time
                if field in ['star_count', 'fork_count']:
                    before_value = before_data.get(field, 0) if before_data else 0
                    after_value = after_data.get(field, reference_value) if after_data else reference_value
                    
                    min_value = min(before_value, after_value)
                    max_value = max(before_value, after_value)
                    
                    actual_int = int(actual_value)
                    if actual_int < min_value or actual_int > max_value:
                        print(f"‚ùå Repo {repo_id}: Field '{field}' out of expected range")
                        print(f"   Expected range: {min_value} - {max_value}")
                        print(f"   Actual: {actual_int}")
                        success = False
                    else:
                        print(f"‚úÖ Repo {repo_id}: Field '{field}' within range ({actual_int} in [{min_value}, {max_value}])")
                
                # Special handling for string fields with normalization
                elif field in ['repo_name', 'owner', 'language']:
                    before_value = before_data.get(field) if before_data else None
                    after_value = after_data.get(field) if after_data else None
                    
                    # Collect valid values (before and after)
                    valid_values = set()
                    if before_value is not None:
                        valid_values.add(normalize_str(str(before_value)))
                    if after_value is not None:
                        valid_values.add(normalize_str(str(after_value)))
                    
                    actual_normalized = normalize_str(str(actual_value)) if actual_value is not None else None
                    
                    if actual_normalized not in valid_values and valid_values:
                        print(f"‚ùå Repo {repo_id}: Field '{field}' mismatch")
                        print(f"   Expected (normalized): {valid_values}")
                        print(f"   Actual (normalized): {actual_normalized}")
                        success = False
                    elif valid_values:
                        print(f"‚úÖ Repo {repo_id}: Field '{field}' matches expected value")
                
                # For description, allow None/empty or exact match
                elif field == 'description':
                    before_value = before_data.get(field) if before_data else None
                    after_value = after_data.get(field) if after_data else None
                    
                    # Normalize descriptions
                    valid_descriptions = set()
                    if before_value is not None:
                        valid_descriptions.add(normalize_str(str(before_value)))
                    if after_value is not None:
                        valid_descriptions.add(normalize_str(str(after_value)))
                    
                    actual_normalized = normalize_str(str(actual_value)) if actual_value is not None else None
                    
                    # Allow None/empty descriptions or matching normalized descriptions
                    if actual_value is None or actual_value == "" or actual_normalized in valid_descriptions:
                        print(f"‚úÖ Repo {repo_id}: Field '{field}' acceptable")
                    else:
                        print(f"‚ùå Repo {repo_id}: Field '{field}' mismatch")
                        print(f"   Expected (normalized): {valid_descriptions}")
                        print(f"   Actual (normalized): {actual_normalized}")
                        success = False
                
                # Special handling for repo_url - normalize http/https prefixes
                elif field == 'repo_url':
                    before_value = before_data.get(field) if before_data else None
                    after_value = after_data.get(field) if after_data else None
                    
                    # Normalize URLs by removing protocol prefix for comparison
                    def normalize_url(url):
                        if url is None:
                            return None
                        url_str = str(url).lower()
                        # Remove http:// or https:// prefix
                        for prefix in ['https://', 'http://']:
                            if url_str.startswith(prefix):
                                return url_str[len(prefix):]
                        return url_str
                    
                    # Collect valid normalized URLs
                    valid_urls = set()
                    if before_value is not None:
                        valid_urls.add(normalize_url(before_value))
                    if after_value is not None:
                        valid_urls.add(normalize_url(after_value))
                    
                    actual_normalized = normalize_url(actual_value)
                    
                    if actual_normalized not in valid_urls and valid_urls:
                        print(f"‚ùå Repo {repo_id}: Field '{field}' mismatch")
                        print(f"   Expected (normalized): {valid_urls}")
                        print(f"   Actual (normalized): {actual_normalized}")
                        success = False
                    elif valid_urls:
                        print(f"‚úÖ Repo {repo_id}: Field '{field}' matches (URL normalized)")
                
                # Standard exact match for other fields (creation_date)
                else:
                    if actual_value != reference_value:
                        print(f"‚ùå Repo {repo_id}: Field '{field}' mismatch")
                        print(f"   Expected: {reference_value}")
                        print(f"   Actual: {actual_value}")
                        success = False
                    else:
                        print(f"‚úÖ Repo {repo_id}: Field '{field}' matches exactly")
        
        if success:
            print(f"‚úÖ Repo {repo_id}: All data validation passed")
        
        return success
    
    def check_milestone_repos(self, data: Dict[str, Any]) -> bool:
        """Check milestone repository data."""
        success = True
        
        # Check required milestone repos (excluding 1000 unless it's in before/after data)
        required_repos = ["1", "1000000", "1000000000"]
        
        # If before/after data contains repo 1000, we should also check it
        if "1000" in self.before_task_data or "1000" in self.after_task_data:
            required_repos.append("1000")
        
        for repo_id in required_repos:
            if repo_id not in data:
                print(f"‚ùå Missing milestone repo: {repo_id}")
                success = False
            else:
                before_repo_data = self.before_task_data.get(repo_id, {})
                after_repo_data = self.after_task_data.get(repo_id, {})
                
                if before_repo_data or after_repo_data:
                    repo_success = self.validate_repo_data(
                        repo_id, data[repo_id], before_repo_data, after_repo_data
                    )
                    success = success and repo_success
                else:
                    print(f"‚ö†Ô∏è  Repo {repo_id}: No reference data available")
        
        return success
    
    def run_evaluation(self) -> bool:
        """Run the complete evaluation."""
        print("üîç Starting GitHub milestone repository evaluation...")
        print(f"üìÅ Agent file: {self.agent_file_path}")
        if self.before_task_file_path:
            print(f"üìÅ Before task file: {self.before_task_file_path}")
        if self.after_task_file_path:
            print(f"üìÅ After task file: {self.after_task_file_path}")
        print("-" * 60)
        
        # Load data
        data = self.load_data()
        if data is None:
            return False
        
        # Check file format
        if not self.check_file_format(data):
            return False
        
        # Generate after_task.json with current GitHub stats (after agent file validation)
        if not self.generate_after_task_data():
            print("‚ö†Ô∏è  Warning: Failed to generate after_task.json, continuing with existing data")
        
        # Check repo 1000 handling
        repo_1000_ok = self.check_repo_1000_handling(data)
        
        # Check milestone repos
        milestone_repos_ok = self.check_milestone_repos(data)
        
        # Summary
        print("-" * 60)
        overall_success = repo_1000_ok and milestone_repos_ok
        
        if overall_success:
            print("üéâ EVALUATION PASSED: All requirements met!")
            print("‚úÖ File format correct")
            print("‚úÖ Repo ID 1000 properly handled")
            print("‚úÖ Milestone repository data accurate")
            if self.before_task_file_path or self.after_task_file_path:
                print("‚úÖ Data within expected before/after task ranges")
            else:
                print("‚úÖ Data matches fallback reference")
        else:
            print("‚ùå EVALUATION FAILED: Some requirements not met")
            if not repo_1000_ok:
                print("   - Repo ID 1000 handling issue")
            if not milestone_repos_ok:
                print("   - Milestone repository data issues")
        
        return overall_success

def main():
    """Main function to run the evaluation."""
    parser = ArgumentParser()
    parser.add_argument("--agent_workspace", required=True, help="AgentÂ∑•‰ΩúÁ©∫Èó¥Ë∑ØÂæÑ")
    parser.add_argument("--groundtruth_workspace", required=False, help="Ground truthÂ∑•‰ΩúÁ©∫Èó¥Ë∑ØÂæÑ")
    parser.add_argument("--res_log_file", required=False, help="ÁªìÊûúÊó•ÂøóÊñá‰ª∂Ë∑ØÂæÑ")
    args = parser.parse_args()
    
    # Construct file paths
    agent_file_path = os.path.join(args.agent_workspace, "github_info.json")
    before_task_file_path = None
    after_task_file_path = None
    
    # load
    before_task_file_path = os.path.join(args.groundtruth_workspace, "before_task.json")
    after_task_file_path = os.path.join(args.groundtruth_workspace, "after_task.json")
    
    # Check if agent file exists
    if not os.path.exists(agent_file_path):
        print(f"‚ùå Error: Agent file not found: {agent_file_path}")
        sys.exit(1)
    
    # Check if before/after task files exist (if specified)
    if before_task_file_path and not os.path.exists(before_task_file_path):
        print(f"‚ö†Ô∏è  Warning: Before task file not found: {before_task_file_path}")
        before_task_file_path = None
    
    if after_task_file_path and not os.path.exists(after_task_file_path):
        print(f"‚ö†Ô∏è  Warning: After task file not found: {after_task_file_path}")
        after_task_file_path = None
    
    if not before_task_file_path and not after_task_file_path:
        print("‚ö†Ô∏è  No before/after task files found. Will use fallback hardcoded data")
    
    # Run evaluation
    evaluator = GitHubRepoEvaluator(agent_file_path, before_task_file_path, after_task_file_path)
    success = evaluator.run_evaluation()
    
    # # Log results if specified
    # if args.res_log_file:
    #     with open(args.res_log_file, 'w') as f:
    #         f.write("PASSED" if success else "FAILED")
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
