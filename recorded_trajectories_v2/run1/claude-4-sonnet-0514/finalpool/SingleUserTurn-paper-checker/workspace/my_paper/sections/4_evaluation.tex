\section{How to evaluate tool use?}
\label{sec:good-tool}

In this section, we study existing LM-tooling benchmarks (\S\ref{sub:testbeds}) and their evaluation metrics (\S\ref{sub:eval-metrics}), lastly, we discuss the missing yet important evaluation aspects of tools (\S\ref{sub:desired-properties}).

% ############
\subsection{Testbeds for evaluating tools}
\label{sub:testbeds}
LM tool use can be evaluated on (i) repurposed existing datasets that can additionally benefit from tools (\S\ref{sub:existing-dataset}), and (ii) newly crafted benchmarks that necessitate tool use (\S\ref{sub:api-benchmark}).

% ############
\subsubsection{Repurposed Existing Datasets}
\label{sub:existing-dataset}
Many tasks are solvable by using LMs, yet often with great difficulty or inefficiency. Therefore, some works use tool-augmented LMs as an alternative approach to solve these tasks. 

Many of these datasets require \textbf{reasoning}. Starting from when questions are expressed in NL, such as complex reasoning with the Big-bench \citep{srivastava2023beyond} dataset, mathematical problems with the MATH \citep{hendrycks2021measuring} dataset, and reasoning over world knowledge to answer questions in NaturalQuestions \citep{kwiatkowski2019natural} and TriviaQA \citep{joshi2017triviaqa} datasets.
Beyond free-form texts, datasets that require reasoning over \textbf{structured data} can also benefit from tools. These tasks include table-based QA with tabular math world problems in TabMWP \citep{lu2023dynamic}, Wikipedia tables in WTQ \citep{pasupat-liang-2015-compositional}, and complex-structured tables in HiTab \citep{cheng-etal-2022-hitab}.
Beyond the text modality, datasets that require reasoning over \textbf{other modalities} also benefit from modality-extending tools, e.g., answering questions about an image with the GQA \citep{hudson2019gqa} dataset, or image pairs with the NLVR2 dataset \citep{suhr2019corpus}.

Because tool use is proposed as an alternative method to solve these datasets, evaluations of these tool-augmented systems follow the standard evaluation process for individual datasets. Concretely, almost all tasks are measured by answer exact match, either in textual or numerical formats.
Note that, to obtain the final answers for lexical matching evaluations, all tool-calling expressions \textbf{need to be executed}, and the execution outputs are incorporated into the final answers produced by the tool-augmented systems, as introduced in \S\ref{sec:basic-paradigm}.

% ############
\subsubsection{Aggregated API Benchmarks}
\label{sub:api-benchmark}
Existing benchmarks can only benefit from a limited set of tools, yet there are far more tools we can utilize to perform versatile tasks in the real world, particularly the API tools created by human developers spread on the web. 
Therefore, many recent works aggregate API tools from various web sources and create benchmarks for using these APIs, as shown in \autoref{tab:api-benchmarks}.

\begin{table}[ht]
\small
\vspace{-1mm}
\begin{center}
    \begin{tabular}{l|lllc}
    \toprule
    \multicolumn{1}{c|}{\textbf{Benchmark}} & \multicolumn{1}{c}{\textbf{Tool Source}} & \multicolumn{1}{c}{\textbf{Example Curation}} & \textbf{Domain (\S\ref{sub:app-spec})} & \multicolumn{1}{c}{\textbf{Executable}} \\ 
    \midrule
    {\hyperlink{cite.xu2023tool}{ToolBench$_1$}} & {existing dataset} & {adopted, human annotated} & {\work, \world} & {\ding{51}} \\
    {\hyperlink{cite.qin2023toolllm}{ToolBench$_2$}} & {RapidAPI} & {model synthesized} & {\work, \world} & {\ding{51}} \\
    {\hyperlink{cite.zhuang2023toolqa}{ToolQA}} & {existing dataset} & {model synthesized} & {\work, \kn} & {\ding{51}} \\
    {\hyperlink{cite.tang2023toolalpaca}{ToolAlpaca}} & {PublicAPIs} & {model synthesized} & {\kn, \work, \world, \modal} & {\ding{55}} \\
    {\hyperlink{cite.li-etal-2023-api}{API-Bank}} & {PublicAPIs} & {human annotated} & {\work, \world} & {\ding{51}} \\
    {\hyperlink{cite.huang2024metatool}{MetaTool}} & {OpenAI Plugins} & {model synthesized} & {\work, \world, \modal} & {\ding{55}} \\
    % \midrule
    {\hyperlink{cite.patil2023gorilla}{Gorilla}} & {HF, Torch, TF} & {model synthesized} & {\nn} & {\ding{55}} \\
    {\hyperlink{cite.shen2023hugginggpt}{HuggingGPT}} & {HF} & {human annotated} & {\nn} & {$~~$\ding{55}$^{*}$}\\
    {\hyperlink{cite.shen2023taskbench}{Task Bench}} & {HF, PublicAPIs} & {model synthesized} & {\nn, \modal, \world} & {\ding{55}} \\
    \bottomrule
    \end{tabular}
\end{center}
\vspace{-3mm}
\caption{Benchmarks of providing aggregated APIs to LMs as tools. 
HF is short for HuggingFace. `\ding{55}$^{*}$' means that: though tools employed by HuggingGPT are executable, it does not evaluate the execution output due to the cost of hosting and inferencing.}
\vspace{-1mm}
\label{tab:api-benchmarks}
\end{table}



\noindent \textbf{Tool sources} \quad
Tools are mainly aggregated from existing datasets or public APIs.
While \citet{xu2023tool,zhuang2023toolqa} adopt existing datasets and propose alternative methods via tool augmentation, these benchmarks are limited in domains. Several works scrape more APIs from online sources such as Public APIs \citep{tang2023toolalpaca}, RESTful APIs \citep{tang2023toolalpaca}, or the OpenAI plugin list \citep{huang2024metatool}. Beyond human-crafted APIs \citep{li-etal-2023-api}, neural models from ML platforms can be similarly presented in an API format \citep{patil2023gorilla,shen2023hugginggpt,shen2023taskbench}. 
Nonetheless, as tools are collected from heterogeneous sources, it is challenging to select the best benchmark or unify all these varied benchmarks.


\noindent \textbf{Example curation} \quad
Examples can be adopted from existing datasets, annotated by humans, or synthesized by LMs.
While most examples adopted from existing datasets are human annotated \citep{xu2023tool}, only \citet{li-etal-2023-api} do so for scraped APIs, by surveying 500 people and creating 314 dialogues manually.
Most other works prompt GPT models to synthesize examples \citep{qin2023toolllm,tang2023toolalpaca,shen2023taskbench,zhuang2023toolqa, huang2024metatool}, however, leading to issues of \textit{naturalness} and \textit{executability}.

\textbf{First}, LMs are often asked to create examples, even tool outputs in \citet{tang2023toolalpaca}, given a heuristically selected set of tools. This approach leads to potential issues in two-fold: (i) the selected tools may not be used together in practice, and (ii) the synthesized examples may not reflect the \textit{natural use cases} of these tools.
\textbf{Second}, 5 out of 9 benchmarks in \autoref{tab:api-benchmarks} do not support tool execution, to alleviate the cost of hosting multiple APIs, especially when they may fail or produce unstable outputs. For example, the weather returned by the \texttt{check\_weather} API may change over time. This un-executability causes \textit{issues in evaluation}. Instead of matching final execution results using lexical- \citep{li-etal-2023-api} or neural-based metrics \citep{tang2023toolalpaca, qin2023toolllm}, works with unexecutable tools resort to pseudo matching of API calling expressions with lexical~\citep{tang2023toolalpaca,shen2023hugginggpt,huang2024metatool} and syntactical \citep{patil2023gorilla, shen2023taskbench} means.



% #################################
\subsection{What metrics are measured now?}
\label{sub:eval-metrics}

\noindent \textbf{Task completion} \quad
Tools are used to assist task solving. Most works that allow tool execution evaluate the task completion score to quantify the effectiveness of utilizing tools.

\noindent \textbf{Tool selection} \quad
For datasets with execution issues \citep{huang2024metatool,shen2023taskbench}, another common metric is the accuracy of selecting the correct tools. This helps disentangle incorrect tool selection errors from inaccurate tool usage errors. 
Despite that tool selection mainly serves as a proxy for evaluating task completion when having unexecutable tools, it can be seen as a measure of LM planning abilities --- the process of breaking down a task into multiple steps and selecting tools to complete individual steps.

\noindent \textbf{Tool reusability} \quad
While tool reusability is often deemed important in took-making literature \citep{cai2023large,yuan2023craft}, only \citet{wang2024trove} evaluates tool reusability by the size of induced toolboxes over a fixed number of examples. As its literal meaning, reusable tools can be (re)used to solve multiple examples hence having more generic functionalities. Adopting a reusable tool is more efficient than using multiple specific tools, and facilitates human verification in both speed and accuracy dimensions \citep{wang2024trove}.


% #################################
\subsection{What properties are missing?}
\label{sub:desired-properties}

\noindent \textbf{Efficiency of tool integration} \quad
As demonstrated by our empirical study (\S\ref{sub:trade-off}), the benefits brought by the tools come with the cost of additional computation, especially for teaching LMs to use tools via training or prompting. In addition to performance gain, reporting the computation overhead can enable fairer comparisons between different approaches.

\noindent \textbf{Quality of tools} \quad
While existing works mostly focus on how tools improve task accuracy, the \textit{performance of tools} themselves is also important. Tool performance can cover multiple aspects such as completing the call quickly, requiring less computation, and not putting users at risk or failing unexpectedly. 
One way to measure these aspects is to conduct API testing \citep{yasar2022software,ehsan2022restful} on their runtime, memory usage, and success rate.

\noindent \textbf{Reliability of unstable tools} \quad
Particularly for tools that involve \textit{neural models} or \textit{randomized components}, their output quality may be unstable and unpredictable. For example, the \texttt{VQA} tool \citep{gupta2022visual} may answer some questions correctly but others incorrectly.
It is important to \textit{be aware of} this uncertainty in contrast to stable, rule-based tools such as a \texttt{calculator}, further alleviate this instability and guarantee more predictable outputs.

\noindent \textbf{Reproducible testing} \quad
Many tools interact with the real world and may return different results at different times. For example, \texttt{check\_weather} may return ``sunny'' today but ``cloudy'' tomorrow. This irreproducible behavior poses great challenges to creating \textit{static evaluation} benchmarks with reference answers. % For instance, the answer to ``How's the weather today?'' should not be a fixed ``sunny'', because the correct answer may change according to the specific time of evaluation.
While some works alleviate this by evaluating API calls without executing them, a more rigorous method could be \textit{parallel testing} \citep{sharma2018automated} --- executing the model-generated program and the reference program in parallel, and measuring if their final outputs match.

\noindent \textbf{Safe usage} \quad
Most systems may only opt to use tools if they are trusted to be secure \citep{barbir2007challenges}.
At the very least, users favor tools that can be easily understood and verified. Further, systems may need to enforce mutual authentication and ensure data integrity \citep{ehsan2022restful}.
Yet there are more security threats and methods beyond the discussion here. We encourage readers to peruse the referenced works above for thorough studies.
