
\documentclass{article} % For LaTeX2e
\PassOptionsToPackage{table,xcdraw,dvipsnames}{xcolor}
\usepackage[preprint]{colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{graphicx}
% \usepackage{color, colortbl}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{xspace}
\usepackage[normalem]{ulem}
\usepackage{enumitem}

% \usepackage[dvipsnames]{xcolor}    % colors

\definecolor{darkblue}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}
\definecolor{lightblue}{RGB}{221,235,247}
\newcommand{\ourmethod}{SimpleRL-Zoo\xspace}
\usepackage[most]{tcolorbox}

\title{SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild}
% \title{SimpleRL: On Emerging Reasoning with Reinforcement Learning for Diverse Open Models -- Insights and Practices}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Weihao Zeng\thanks{Equal Contribution. Correspondence to Weihao Zeng (wzengak@connect.ust.hk), Yuzhen Huang (yhuanghj@cse.ust.hk), and Junxian He (junxianh@cse.ust.hk).}\hspace{4pt}$^1$ \quad Yuzhen Huang$^{*1}$ \quad Qian Liu$^{*2}$ \quad
Wei Liu$^1$ \quad
Keqing He$^3$ \\ \textbf{Zejun Ma$^2$ \quad Junxian He$^1$}\\
$^1$HKUST \quad $^2$TikTok \quad $^3$Meituan\\
\href{https://github.com/hkust-nlp/simpleRL-reason}{https://github.com/hkust-nlp/simpleRL-reason}
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\newcommand{\wz}[1]{\textcolor{brown}{\bf\small [#1 --WZ]}}
\newcommand{\yh}[1]{\textcolor{orange}{\bf\small [#1 --YH]}}
\newcommand{\jh}[1]{\textcolor{magenta}{\bf\small [#1 --JH]}}
\newcommand{\jhc}[2]{\bgroup\textcolor{magenta}{\sout{#1} #2}\egroup}
\newcommand{\qian}[1]{\textcolor{blue}{\bf\small [#1 --Qian]}}


\newcommand{\up}[1]{\textcolor{OliveGreen}{\small \ $\uparrow${#1}}}
\newcommand{\down}[1]{\textcolor{Maroon}{\small \ $\downarrow${#1}}}
\usepackage{arydshln}
\begin{document}

\ifcolmsubmission
\linenumbers
\fi

\maketitle

\begin{abstract}
% Advanced large language model has achieved leading performance
% Long chain-of-thought (CoT) reasoning, with high-level reasoning mechanisms such as reflection and verification, has been shown to be a powerful approach for scaling test-time computation and achieving remarkable performance in extremely complex reasoning tasks.

% Advanced models can perform long chain-of-thought (CoT) reasoning on complex tasks to achieve enhanced performance. 
DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models—a paradigm referred to as \emph{zero RL training}. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities.
In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. 
% can several critical questions remain unanswered: (1) Can smaller, various base models exhibit similar emergent reasoning with limited, simple data? (2) Does increased CoT length always lead to increased cognitive phenomena such as self-reflection (i.e., the``aha moment'')? (3) What are the essential designs that enable the emergence of long CoT? In this work, we take mathematical reasoning as an example and try to answer these questions. We conduct extensive zero-training experiments across various base models, spanning different families and sizes, including LLama3.1-8B, Mistral-7B/24B, DeepSeekMath-7B, and Qwen2.5-0.5B/1.5B/7B/14B/32B. To keep the recipe simple, we restrict our training data to the original GSM8K and MATH datasets. 
Leveraging several key design strategies—such as adjusting format reward and controlling query difficulty—we achieve substantial improvements in both reasoning accuracy and response length across most settings.
However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the ``aha moment"). Notably, we observe the ``aha moment'' for the first time in small models not from the Qwen family.
% Through comprehensive analyses of the results, we uncover several intriguing findings. For example, increased CoT length does not always correspond to the ``aha moment'', and different base models exhibit distinct behaviors during training. 
% We also compare our approach to traditional RL pipelines that follow a traditional SFT stage, demonstrating that such common practices can slow down or even inhibit the emergence of long CoT reasoning. 
We share the key designs that enable successful zero RL training, along with our findings and practices. 
To facilitate further research, we open-source the code, models, and analysis tools.
% \qian{I think we can mention somewhere that we employ the same data and the same hyper-parameter to make it work on every model, and it would be really impressive.}
% \footnote{A preliminary result of this paper based on Qwen2.5-Math-7B was reported in a \href{https://hkust-nlp.notion.site/simplerl-reason?pvs=4}{blog} released concurrently with DeepSeek-R1~\citep{guo2025deepseek}.
% \jh{the deepseek r1 citation seems weird, I think other people cite it where the first author is DeepSeekAI?}
% \jh{this footnote needs to be removed in the submission}}  

% Advanced models can perform long chain-of-thought (CoT) reasoning on complex tasks by leveraging high-level mechanisms like reflection. 
% Recently, open-source flagship long-CoT models such as DeepSeek-R1 have introduced a simple yet effective rule-based reward design, using pure reinforcement learning (RL) directly from base models without any supervised fine-tuning (SFT)—a process known as ``zero training". However, most recent efforts have concentrated on a limited series of models and have not thoroughly analyzed the critical underlying factors contributing to the success of such seemingly simple designs. In this paper, we investigates the mechanisms and challenges associated with ``zero training" in LLM, focusing on the emergence of high-level reasoning mechanisms during RL training. We explore how reasoning behaviors evolve across different series of models ranging in size from 1.5B to 32B parameters, and analyze the critical factors contributing to successful ``zero training". Using limited open-source datasets like GSM8K and MATH, we demonstrate significant performance improvements and increased response lengths, highlighting the emergence of reflection-like reasoning across different models. Through fine-grained monitoring, we uncover key challenges that affect both training stability and overall performance.
% %Through find-grained monitoring, we identify key challenges, including weak zero-shot capabilities, rigid formatting constraints, and misalignment between data difficulty and model capabilities. These factors impact both training stability and performance.  
% Furthermore, we reveal the surprising limitations of traditional SFT methods, which, while effective for short-term performance gains, hinder model exploration and constrain the development of advanced reasoning abilities. Our findings offer essential insights into RL training dynamics and suggest directions for scalable zero-training in the future.
\end{abstract}


% 1. The first comprehensive experiment to perform zero RL training on different base models (apart from the Qwen base model) and conduct in-depth analysis, including analysis of length and performance, using comprehensive metrics (including pass@k) to analyze length changes, and whether reflection is produced (in this experimental setting, we need to align most of the settings). **TODO:** We need to quickly align key settings, including whether to remove formatting, the difficulty level of prompts used, and finalize the experiments.
% 2. Analysis of key tricks affecting zero experiment effectiveness, including stop ids, reward design (with or without format), prompt difficulty design, and other key hyperparameters such as batch size/KL coefficient/temperature. **TODO:** We need to select 2-3 representative models for analysis experiments
% 3. Comprehensive analysis of why base models may or may not produce emergence, including patterns of emergence in base models, pass@K factors. Establish connections between base models and the aha momentTODO:
% 4. Explain why traditional SFT experiments don't work? For example, why might using short CoT for SFT be harmful? (Expected conclusion: using short CoT for SFT suppresses emergence) TODO: Also conduct SFT on the representative models from point 2 for analysis experiments


\input{introduction}

\input{Zero_Result}

%\section{Dynamics of Emergence in Zero Training}

%  \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\vspace{-10pt}
%     \caption{Comparison of accuracy and response length with and without format rewards.}
%     \label{fig:w&wo_format_reward}
%     \vspace{-20pt}
% \end{wrapfigure}


\section{Key Factors Shaping Zero RL Training}
In this section, we identify key factors that influence stability and performance during zero RL training, particularly when dealing with early-stage or weaker models.  
First, we explore how an over-reliance on format rewards restricts exploration. Next, we analyze how data difficulty impacts exploratory behavior, illustrating how exposure to varying levels of difficulty shapes the exploration dynamics. We also discuss the impact of exploration-related hyperparameters in Appendix~\ref{sec:impact_explore_hyper}.


%  \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\vspace{-10pt}
%     \caption{Comparison of accuracy and response length with and without format rewards.}
%     \label{fig:w&wo_format_reward}
%     \vspace{-20pt}
% \end{wrapfigure}



%\subsection{Inherent Limitations of Zero-Shot Capabilities}


% 感觉可以包含两个小部分，一个是prompt的影响，另一个是stop id的实验

% 对于prompt的影响，可以直接在0.5b上做
% 对于stop id的影响，则在mistral上做 (取消)
% \begin{figure*}[!t]
%     \centering
%     % \subfigure[Pass@1 on GSM8K]{
%     %     \includegraphics[width=0.23\textwidth]{fig/gsm8k_pass@1_math_only_v2.pdf}
%     % }
%     \subfigure[Qwen-2.5-7B]{
%         \includegraphics[width=0.4\textwidth]{fig/plot_figure5_v1-3_w&wo_format_reward-qwen-2.5-7b.pdf}
%         \label{fig:w&wo_format_reward-qwen-2.5-7b}
%     }
%     \subfigure[Llama-3.1-8B]{
%         \includegraphics[width=0.4\textwidth]{fig/plot_figure5_v1-4_w&wo_format_reward-llama-3.1-8b.pdf}
%         \label{fig:w&wo_format_reward-llama-3.1-8b}
%     }\vspace{-10pt}
%     \caption{Comparison of accuracy and response length with and without format rewards.}
%     \label{fig:w&wo_format_reward}
%     \vspace{-10pt}
% \end{figure*}

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\vspace{-10pt}
%      \caption{Comparison of accuracy and response length with and without format rewards.}
%    \label{fig:w&wo_format_reward}
% \end{figure*}


% \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\vspace{-10pt}
%     \caption{Comparison of accuracy and response length with and without format rewards.}
%     \label{fig:w&wo_format_reward}
%     \vspace{-10pt}
% \end{wrapfigure}

% \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \subfigure[Qwen-2.5-7B]{
%         \includegraphics[width=0.21\textwidth]{fig/plot_figure5_v1-3_w&wo_format_reward-qwen-2.5-7b.pdf}
%         \label{fig:w&wo_format_reward-qwen-2.5-7b}
%     }
%     \subfigure[Llama-3.1-8B]{
%         \includegraphics[width=0.21\textwidth]{fig/plot_figure5_v1-4_w&wo_format_reward-llama-3.1-8b.pdf}
%         \label{fig:w&wo_format_reward-llama-3.1-8b}
%     }\vspace{-10pt}
%     \caption{Comparison of accuracy and response length with and without format rewards.\yh{smaller with Figure 10}}
%     \label{fig:w&wo_format_reward}
% \end{wrapfigure}


%  \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\vspace{-10pt}
%     \caption{Comparison of accuracy and response length with and without format rewards.}
%     \label{fig:w&wo_format_reward}
%     \vspace{-20pt}
% \end{wrapfigure}

\begin{figure}[!t]
        \centering
\includegraphics[width=\columnwidth]{fig/plot_figure4_v4_behavior_lines.pdf}
%\vspace{-10pt}
\caption{The change in reasoning behavior over the training iterations across all models. As described in \S\ref{sec:eval_metrics}, we use GPT-4o to extract and track shifts in reasoning behaviors on OlympiadBench. We focus on four reasoning-related behaviors: ``Backtracking", ``Verification", ``Subgoal Setting", and ``Enumeration".
        }
        \label{fig4:behavior&counts}
    %\vspace{-15pt}
\end{figure}






% 基本上可以照着做不work的角度走了
% First, they often struggle to accurately interpret complex instructions, such as formatting answers within specific boxes. Second, many base models tend to generate sequences continuously, failing to stop once the question has been answered. These issues lead to a high volume of irrelevant outputs in the early stages of training, contributing to model collapse, where the models get stuck in repetitive, meaningless loops.  To address this, we systematically analyze the impact of such behavior and propose straightforward yet effective solutions to mitigate these problems. \wz{This content is more about tricks, it would be better to move it to the appendix or remove it}




\subsection{Over-Reliance on Format Rewards}

We find that enforcing strict formatting constraints, such as requiring the final answer to be enclosed in a latex command \textit{\textbackslash boxed\{\}}, can hinder model's freely exploration and ultimately degrades performance. This is because many base models cannot follow the format constraint well in the initial stage, and imposing a format reward will penalize many correct explorations. We compare two reward functions: one without format constraints, which rewards responses solely based on answer correctness (our default design in \textsection\ref{sec:reward_remove}), and another that strictly enforces formatting by penalizing responses with a reward of -1 if they fail to adhere to the required format.


%  \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\vspace{-10pt}
%     \caption{Comparison of accuracy and response length with and without format rewards.}
%     \label{fig:w&wo_format_reward}
%     \vspace{-20pt}
% \end{wrapfigure}

\label{sec:remove_format}
 \begin{wrapfigure}{r}{0.48\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\vspace{-10pt}
    \caption{Accuracy and response length with and without format rewards.}
    \label{fig:w&wo_format_reward}
    \vspace{-5pt}
\end{wrapfigure}


Figure~\ref{fig:w&wo_format_reward} illustrates weaker models like Llama-3.1-8B struggle under strict formatting requirements, leading to a rapid increase in response length early in training without performancec improvement. The model expends excessive effort on adhering to the format but fails to learn how to answer correctly, ultimately resulting in model collapse. 
Figure~\ref{fig:w&wo_format_reward} (Left) further reveals that even stronger models, such as Qwen-2.5-7B, which initially comply with formatting constraints, suffer in later training stages. This includes both performance degradation and a significant reduction in CoT length.  
%  \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\vspace{-10pt}
%     \caption{Comparison of accuracy and response length with and without format rewards.}
%     \label{fig:w&wo_format_reward}
%     \vspace{-20pt}
% \end{wrapfigure}
% \wz{Here the total length with format reward is much longer, I'm wondering if we need to explain this using other metrics}
These findings highlight that: in a zero RL training setting, rather than imposing rigid formatting rules, we should prioritize maintaining response verifiability while allowing sufficient flexibility for exploration.

% 影响性能上限和reasonng behaviur的涌现
\begin{figure*}[!t]
    \centering
    % \subfigure[Pass@1 on GSM8K]{
    %     \includegraphics[width=0.23\textwidth]{fig/gsm8k_pass@1_math_only_v2.pdf}
    % }
    \subfigure[Mistral-7b-v0.1]{
        \includegraphics[width=0.47\textwidth]{fig/plot_figure6_v3-2_mathlv-mistral-7b-v0.1.pdf}
        \label{fig:mathlv-mistral-7b-v0.1}
    }
    \subfigure[Qwen-2.5-7B]{
        \includegraphics[width=0.47\textwidth]{fig/plot_figure6_v3_mathlv-qwen-2.5-7b.pdf}
        \label{fig:mathlv-qwen-2.5-7b}
    }
    %\vspace{-10pt}
    \caption{Comparison of accuracy and response length across different data difficulty levels. We examine three levels of data: Easy (GSM8K and MATH lv.1), Medium (MATH lv.1–4), and Hard (MATH lv.3–5), with each category containing approximately 8,000 problems.}
    \label{fig:mathlv}
    %\vspace{-15pt}
\end{figure*}
\subsection{Data Difficulty on Exploratory Behavior}
\label{sec:data_complextiy_behaviur}
Base models exhibit varying performance and CoT behaviors when trained on different RL data. Figure~\ref{fig:mathlv} compare the performance of Mistral-7B and Qwen-2.5-7B across Easy, Medium, and Hard datasets. We observe a clear trend: as data difficulty increases, Mistral-7B's performance progressively deteriorates. When faced with high-difficulty data (Hard: MATH levels 3-5), the model struggles to generate responses that receive positive feedback from the reward system. This failure results in a significant increase in response length without any corresponding improvement in accuracy, signaling a breakdown in the training process—often referred to as training collapse. Figure~\ref{fig:mathlv} Left demonstrates that Qwen-2.5-7B exhibits a pattern entirely opposite to Mistral-7B-v0.1. Specifically, as dataset difficulty decreases, both the model’s average accuracy and response length decline, with the effect being most pronounced on the simplest dataset, where even response length decreases. This finding aligns with our previous analysis of Qwen-2.5-7B in \textsection\ref{sec:qwen_behabiur}, reinforcing the notion that Qwen inherently possesses strong reasoning capabilities. To further improve its response length, training should incorporate more challenging datasets to encourage deeper reasoning and extended thinking time.
The analysis highlights that zero RL training data must align with the base model's inherent reasoning capabilities.

% \subsection{Impact of Exploration-Related Hyperparameters}
% \jh{this entire section can go to appendix given that the paper is already long}
% In this section, we examine the effects of exploration-related hyperparameters on "zero-training." Drawing inspiration from ~\citet{zeng2024b}, we focus on two key factors: sampling size (the number of responses per query) and sampling temperature.

% \paragraph{Sampling Size\jhc{}{:}} We examine how varying sampling sizes $N \in \{1,4,8,16,32\}$ influence the training process using the Mistral 24B model; these results are presented in Figure~\ref{fig:sampling_number}. Our analysis reveals a clear trend: as $N$ increases, the model's average performance notably improves, and variability in response lengths becomes significantly more stable. For example, after 100 training steps, the scenario with $N=32$ achieves an average accuracy approximately 6 points higher than that with $N=8$. Conversely, smaller sampling sizes ($N=1$ and $N=4$) cause training instability and potential collapse, indicated by rapid growth in generated length without corresponding accuracy improvements. We hypothesize that larger sample sizes enable the model to explore a broader and more diverse training space, which stabilizes advantage estimation and sustains continuous performance improvement.





% \begin{figure}[!t]
%     % 左侧图片
%     \begin{minipage}[t]{0.48\columnwidth} % 留2%间隙
%         \centering
%         \includegraphics[width=\linewidth]{fig/plot_figure8_v3_diff_n-mistral-24b}\vspace{-10pt}
%         \caption{Comparison of accuracy and response length using different sampling numbers N = 1, 4, 8, 32. The training data is the Hard part (MATH lv.3–5) with the same setting in main results, as described in \S~\ref{sec:setup}.}
%         \label{fig:sampling_number}
%     \end{minipage}
%     \hfill % 填充间隙
%     % 右侧图片
%     \begin{minipage}[t]{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/plot_temperature.pdf}
%         \caption{Impact of training and evaluation temperatures on Qwen-2.5-0.5b's average final performance (x-axis: evaluation temp, y-axis: training temp).}
%         \label{fig:train_sample_temperature}
%     \end{minipage}
% \end{figure}

% \paragraph{Sampling Temperature\jhc{}{:}} We conduct research on Qwen-2.5-0.5B to analyze the impact of sampling temperature during both training and evaluation on model performance. The results, presented in Figure~\ref{fig:train_sample_temperature} , indicate that training with higher temperatures generally leads to better average performance. For instance, models trained with temperatures of 1.0 and 1.2 outperform those trained with 0.8 and 0.6. Additionally, we find that the optimal evaluation temperature depends on the training temperature. Specifically, models trained at higher temperatures require higher sampling temperatures during evaluation, as using greedy sampling often results in repetitive outputs. Conversely, models trained at lower temperatures perform best when evaluated with lower sampling temperatures.


\section{Revisiting Traditional SFT for RL-Driven Reasoning Emergence}

\label{sec:short_cot_influence}




\begin{figure}[!t]
        \centering
\includegraphics[width=0.97\columnwidth]{fig/plot_figure10_v1_behaviors.pdf}\vspace{-10pt}
\caption{Reasoning behavior ratio over RL training iterations after using different SFT steps as starting points. ``Base" refers to the base Mistral-Small-24B model without any SFT, while ``Step 100" and ``Step 500" represent 100 and 500 steps of SFT, respectively. As described in \S\ref{sec:setup}, we use GPT-4o to track shifts in reasoning behaviors on OlympiadBench.}
        %\vspace{-15pt}
         \label{fig:sft_behaviur}
\end{figure}



As base models may not follow instruction well and pose a major challenge for zero RL training, one may wonder a simple SFT stage as a cold start may be helpful to learn to follow instructions well. In this section, we revisit the impact of traditional SFT methods (where the responses are not from long CoT models) as a cold start on RL training performance and reasoning behavior--notably, this was the most commonly used post-training pipeline with RL following an SFT stage, before DeepSeek-R1. Specifically, we use a subset of the NuminaMath~\citep{li2024numinamath} dataset derived from GSM8K and MATH,~\footnote{We also conduct experiments using general SFT dataset beyond math-related ones, which can be found in Appendix~\ref{sec:general_sft} and implies similar conclusion.} containing approximately 15K high-quality short CoT responses. We conduct SFT using Mistral 24B and select models at 100 and 500 training steps as starting points for RL training. 
% \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure9_v2_diff_n-mistral-24b.pdf}
%     %\vspace{-10pt}
%     \caption{Accuracy and response length averaged on the six benchmarks over RL training iterations after running different SFT steps as starting points. }
%     \label{fig:sft_accuracy_length} 
%     %\vspace{-10pt}
% \end{wrapfigure}

Figure~\ref{fig:sft_accuracy_length} illustrates how model accuracy and output length evolve during RL training when different initial models are used. Our results indicate that starting from SFT models initially boosts performance significantly; however, these models encounter notable limitations in their maximum achievable accuracy and response length compared to starting from the base model during RL training. Crucially, we observe that these limitations become increasingly pronounced as the number of initial SFT steps grows.


\begin{wrapfigure}{r}{0.48\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{fig/plot_figure9_v2_diff_n-mistral-24b.pdf}
    %\vspace{-10pt}
    \caption{Accuracy and response length averaged on the six benchmarks over RL training iterations after running different SFT steps as starting points. }
    \label{fig:sft_accuracy_length} 
    %\vspace{-10pt}
\end{wrapfigure}

%For instance, while the base model can attain a pass@1 accuracy of approximately 49.6\% during RL training, models initialized with 100 and 500 SFT steps achieve maximum accuracies of only about 47.3\% and 40.3\%, respectively.

%For instance, \jh{need to cite the table here}while the base model can attain a pass@1 accuracy of approximately \jhc{72}{72\%} on MATH-500 during RL training, models initialized with 100 and 500 SFT steps achieve maximum accuracies of only about \jhc{68}{68\%} and \jhc{59}{59\%}, respectively.

To further investigate how initial SFT affects the emergence of reasoning behaviors, we analyze how often specific reasoning behaviors appeared during training at different starting points, as shown in Figure~\ref{fig:sft_behaviur}. Our analysis reveals that initial SFT negatively impacts the development of critical reasoning behaviors. Specifically, models with 100 SFT steps exhibit reduced upper limits in essential reasoning behaviors such as "enumeration," "verification," and "backtracking," compared to the base model. Even more notably, models with 500 SFT steps experience significant declines in "enumeration" and "verification" behaviors in later training stages, highlighting a detrimental long-term effect of extensive sft on reasoning capabilities.
% \begin{wrapfigure}{r}{0.48\columnwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/plot_figure9_v2_diff_n-mistral-24b.pdf}\vspace{-10pt}
%     \caption{Accuracy and response length averaged on the six benchmarks over RL training iterations after running different SFT steps as starting points. ``Base" refers to the base Mistral-Small-24B model without any SFT, while ``Step 100" and ``Step 500" represent 100 and 500 steps of SFT on the base model, respectively.}
%     \label{fig:sft_accuracy_length}
% \end{wrapfigure}
This prompts a reconsideration of whether traditional SFT inherently restricts model exploration, perhaps highlighting the need for future cold-start strategies to prioritize exploration capacity—whether by incorporating long CoT data~\citep{guo2025deepseek,yeo2025demystifying} or designing SFT techniques~\citep{li2025preserving} that strike a balance between imitation and exploration—to enable sustained improvements in model reasoning performance.

\section{Conclusion}
Our paper demonstrates the effectiveness of zero RL training across a diverse range of base models, yielding significant improvements in accuracy and response length. We provide strong evidence that zero RL training is not merely reranking, but rather a genuine enhancement. Furthermore, we identify key factors such as reward design, data difficulty, and models' inherent abilities that shape the emergence of advanced reasoning behaviors. Our findings also indicate that starting RL training from models with traditional SFT may limit the development of advanced reasoning behaviors. Overall, our work highlights key factors for effective zero RL training and offers insights for future model improvements.

% \section*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \section*{Acknowledgments}
% Use unnumbered first level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

% \section*{Ethics Statement}
% Authors can add an optional ethics statement to the paper. 
% For papers that touch on ethical issues, this section will be evaluated as part of the review process. The ethics statement should come at the end of the paper. It does not count toward the page limit, but should not be more than 1 page. 



\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}
\newpage
\input{Appendix}
\end{document}